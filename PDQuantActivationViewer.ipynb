{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Using Pytorch Dataset\n",
      "Setting the first and the last layer to 8-bit\n",
      "the quantized model is below!\n",
      "QuantModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): QuantModule(\n",
      "      wbit=8, abit=4, disable_act_quant=False\n",
      "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
      "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      (norm_function): StraightThrough()\n",
      "      (activation_function): ReLU(inplace=True)\n",
      "    )\n",
      "    (bn1): StraightThrough()\n",
      "    (relu): StraightThrough()\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "      (1): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (downsample): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "      (1): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (downsample): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "      (1): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (downsample): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      )\n",
      "      (1): QuantBasicBlock(\n",
      "        (conv1): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=False\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): ReLU(inplace=True)\n",
      "        )\n",
      "        (conv2): QuantModule(\n",
      "          wbit=4, abit=4, disable_act_quant=True\n",
      "          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "          (norm_function): StraightThrough()\n",
      "          (activation_function): StraightThrough()\n",
      "        )\n",
      "        (activation_function): ReLU(inplace=True)\n",
      "        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): QuantModule(\n",
      "      wbit=8, abit=4, disable_act_quant=True\n",
      "      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)\n",
      "      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)\n",
      "      (norm_function): StraightThrough()\n",
      "      (activation_function): StraightThrough()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Reconstruction for layer conv1\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 0\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 1\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 0\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 1\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 0\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 1\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 0\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for block 1\n",
      "Init alpha to be FP32\n",
      "Init alpha to be FP32\n",
      "Reconstruction for layer fc\n",
      "Init alpha to be FP32\n",
      "Test: [  0/782]\tTime  8.446 ( 8.446)\tAcc@1  87.50 ( 87.50)\tAcc@5  96.88 ( 96.88)\tInferenceSpeed (ms)  8.446 ( 8.446)\n",
      "Test: [100/782]\tTime 15.515 ( 9.198)\tAcc@1  82.81 ( 76.42)\tAcc@5  98.44 ( 92.14)\tInferenceSpeed (ms) 15.515 ( 9.198)\n",
      "Test: [200/782]\tTime  5.839 ( 9.162)\tAcc@1  75.00 ( 75.65)\tAcc@5  93.75 ( 93.39)\tInferenceSpeed (ms)  5.839 ( 9.162)\n",
      "Test: [300/782]\tTime  6.008 ( 9.018)\tAcc@1  76.56 ( 75.87)\tAcc@5  98.44 ( 93.43)\tInferenceSpeed (ms)  6.008 ( 9.018)\n",
      "Test: [400/782]\tTime  5.965 ( 9.203)\tAcc@1  67.19 ( 72.92)\tAcc@5  95.31 ( 91.71)\tInferenceSpeed (ms)  5.965 ( 9.203)\n",
      "Test: [500/782]\tTime  5.661 ( 9.077)\tAcc@1  78.12 ( 71.36)\tAcc@5  95.31 ( 90.53)\tInferenceSpeed (ms)  5.661 ( 9.077)\n",
      "Test: [600/782]\tTime  5.900 ( 9.102)\tAcc@1  78.12 ( 70.22)\tAcc@5  90.62 ( 89.69)\tInferenceSpeed (ms)  5.900 ( 9.102)\n",
      "Test: [700/782]\tTime  5.885 ( 9.306)\tAcc@1  73.44 ( 69.27)\tAcc@5  89.06 ( 89.07)\tInferenceSpeed (ms)  5.885 ( 9.306)\n",
      " * Acc@1 69.206 Acc@5 89.068 Inference Speed 9.193 ms/batch\n",
      "Full quantization (W4A4) accuracy: 69.20600128173828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import hubconf  # noqa: F401\n",
    "import copy\n",
    "from quant import (\n",
    "    block_reconstruction,\n",
    "    layer_reconstruction,\n",
    "    BaseQuantBlock,\n",
    "    QuantModule,\n",
    "    QuantModel,\n",
    "    set_weight_quantize_params,\n",
    ")\n",
    "from data.imagenet import build_imagenet_data\n",
    "\n",
    "\n",
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_model(val_loader, model, device=None, print_freq=100):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        model.to(device)\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
    "    inference_speed = AverageMeter(\"InferenceSpeed (ms)\", \":6.3f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader), [batch_time, top1, top5, inference_speed], prefix=\"Test: \"\n",
    "    )\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    # model loading...\n",
    "    for i, (images, target) in enumerate(val_loader):\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(images)\n",
    "        break\n",
    "\n",
    "    for i, (images, target) in enumerate(val_loader):\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        end = time.time()\n",
    "        output = model(images)\n",
    "\n",
    "        # measure elapsed time for inference in milliseconds\n",
    "        inference_time = (time.time() - end) * 1000\n",
    "        inference_speed.update(inference_time)\n",
    "        batch_time.update(inference_time)\n",
    "        end = time.time()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    print(\n",
    "        \" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f} Inference Speed {inference_speed.avg:.3f} ms/batch\".format(\n",
    "            top1=top1, top5=top5, inference_speed=inference_speed\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def get_train_samples(train_loader, num_samples):\n",
    "    train_data, target = [], []\n",
    "    for batch in train_loader:\n",
    "        train_data.append(batch[0])\n",
    "        target.append(batch[1])\n",
    "        if len(train_data) * batch[0].size(0) >= num_samples:\n",
    "            break\n",
    "    return (\n",
    "        torch.cat(train_data, dim=0)[:num_samples],\n",
    "        torch.cat(target, dim=0)[:num_samples],\n",
    "    )\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"running parameters\",\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    ")\n",
    "# general parameters for data and model\n",
    "parser.add_argument(\n",
    "    \"--seed\", default=1005, type=int, help=\"random seed for results reproduction\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arch\",\n",
    "    default=\"resnet18\",\n",
    "    type=str,\n",
    "    help=\"model name\",\n",
    "    choices=[\n",
    "        \"resnet18\",\n",
    "        \"resnet50\",\n",
    "        \"mobilenetv2\",\n",
    "        \"regnetx_600m\",\n",
    "        \"regnetx_3200m\",\n",
    "        \"mnasnet\",\n",
    "    ],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=64, type=int, help=\"mini-batch size for data loader\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--workers\", default=4, type=int, help=\"number of workers for data loader\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\",\n",
    "    default=\"/datasets-to-imagenet\",\n",
    "    type=str,\n",
    "    help=\"path to ImageNet data\",\n",
    ")\n",
    "\n",
    "# quantization parameters\n",
    "parser.add_argument(\n",
    "    \"--n_bits_w\", default=4, type=int, help=\"bitwidth for weight quantization\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--channel_wise\",\n",
    "    default=True,\n",
    "    help=\"apply channel_wise quantization for weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_bits_a\", default=4, type=int, help=\"bitwidth for activation quantization\"\n",
    ")\n",
    "parser.add_argument(\"--disable_8bit_head_stem\", action=\"store_true\")\n",
    "\n",
    "# weight calibration parameters\n",
    "parser.add_argument(\n",
    "    \"--num_samples\", default=1024, type=int, help=\"size of the calibration dataset\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--iters_w\", default=20000, type=int, help=\"number of iteration for adaround\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight\",\n",
    "    default=0.01,\n",
    "    type=float,\n",
    "    help=\"weight of rounding cost vs the reconstruction loss.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--keep_cpu\", action=\"store_true\", help=\"keep the calibration data on cpu\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--b_start\",\n",
    "    default=20,\n",
    "    type=int,\n",
    "    help=\"temperature at the beginning of calibration\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--b_end\", default=2, type=int, help=\"temperature at the end of calibration\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--warmup\",\n",
    "    default=0.2,\n",
    "    type=float,\n",
    "    help=\"in the warmup period no regularization is applied\",\n",
    ")\n",
    "\n",
    "# activation calibration parameters\n",
    "parser.add_argument(\"--lr\", default=4e-5, type=float, help=\"learning rate for LSQ\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--init_wmode\",\n",
    "    default=\"mse\",\n",
    "    type=str,\n",
    "    choices=[\"minmax\", \"mse\", \"minmax_scale\"],\n",
    "    help=\"init opt mode for weight\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--init_amode\",\n",
    "    default=\"mse\",\n",
    "    type=str,\n",
    "    choices=[\"minmax\", \"mse\", \"minmax_scale\"],\n",
    "    help=\"init opt mode for activation\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--prob\", default=0.5, type=float)\n",
    "parser.add_argument(\"--input_prob\", default=0.5, type=float)\n",
    "parser.add_argument(\n",
    "    \"--lamb_r\", default=0.1, type=float, help=\"hyper-parameter for regularization\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--T\", default=4.0, type=float, help=\"temperature coefficient for KL divergence\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bn_lr\", default=1e-3, type=float, help=\"learning rate for DC\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lamb_c\", default=0.02, type=float, help=\"hyper-parameter for DC\"\n",
    ")\n",
    "parser.add_argument(\"--filename\", default=\"tmp\", type=str, help=\"name of pth file\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.seed = 1005\n",
    "args.arch = \"resnet18\"\n",
    "args.batch_size = 64\n",
    "args.workers = 8\n",
    "args.data_path = \"data/ImageNet\"\n",
    "\n",
    "# Quantization parameters\n",
    "args.n_bits_w = 4\n",
    "args.channel_wise = True\n",
    "args.n_bits_a = 4\n",
    "\n",
    "# Weight calibration parameters\n",
    "args.num_samples = 1024\n",
    "args.iters_w = 20000\n",
    "args.weight = 0.01\n",
    "\n",
    "args.b_start = 20\n",
    "args.b_end = 2\n",
    "args.warmup = 0.2\n",
    "\n",
    "# Activation calibration parameters\n",
    "args.lr = 4e-5\n",
    "\n",
    "args.init_wmode = \"mse\"\n",
    "args.init_amode = \"mse\"\n",
    "\n",
    "args.prob = 0.5\n",
    "args.input_prob = 0.5\n",
    "args.lamb_r = 0.1\n",
    "args.T = 4.0\n",
    "args.bn_lr = 1e-3\n",
    "args.lamb_c = 0.02\n",
    "\n",
    "args.filename = \"backup/default\"  # Filename to save the model / 24.05.29 @jiho264\n",
    "\n",
    "seed_all(args.seed)\n",
    "# build imagenet data loader\n",
    "train_loader, test_loader = build_imagenet_data(\n",
    "    batch_size=args.batch_size, workers=args.workers, data_path=args.data_path\n",
    ")\n",
    "# load model\n",
    "cnn = eval(\"hubconf.{}(pretrained=True)\".format(args.arch))\n",
    "cnn.cuda()\n",
    "cnn.eval()\n",
    "\n",
    "# print(\"Full Precision accuracy: {}\".format(validate_model(test_loader, cnn)))\n",
    "\n",
    "fp_model = copy.deepcopy(cnn)\n",
    "fp_model.cuda()\n",
    "fp_model.eval()\n",
    "\n",
    "# build quantization parameters\n",
    "wq_params = {\n",
    "    \"n_bits\": args.n_bits_w,\n",
    "    \"channel_wise\": args.channel_wise,\n",
    "    \"scale_method\": args.init_wmode,\n",
    "}\n",
    "aq_params = {\n",
    "    \"n_bits\": args.n_bits_a,\n",
    "    \"channel_wise\": False,\n",
    "    \"scale_method\": args.init_amode,\n",
    "    \"leaf_param\": True,\n",
    "    \"prob\": args.prob,\n",
    "}\n",
    "\n",
    "fp_model = QuantModel(\n",
    "    model=fp_model,\n",
    "    weight_quant_params=wq_params,\n",
    "    act_quant_params=aq_params,\n",
    "    is_fusing=False,\n",
    ")\n",
    "fp_model.cuda()\n",
    "fp_model.eval()\n",
    "fp_model.set_quant_state(False, False)\n",
    "qnn = QuantModel(\n",
    "    model=cnn, weight_quant_params=wq_params, act_quant_params=aq_params\n",
    ")\n",
    "qnn.cuda()\n",
    "qnn.eval()\n",
    "if not args.disable_8bit_head_stem:\n",
    "    print(\"Setting the first and the last layer to 8-bit\")\n",
    "    qnn.set_first_last_layer_to_8bit()\n",
    "\n",
    "qnn.disable_network_output_quantization()\n",
    "print(\"the quantized model is below!\")\n",
    "print(qnn)\n",
    "# cali_data, cali_target = get_train_samples(\n",
    "#     train_loader, num_samples=args.num_samples\n",
    "# )\n",
    "cali_data, cali_target = None, None\n",
    "device = next(qnn.parameters()).device\n",
    "\n",
    "# Kwargs for weight rounding calibration\n",
    "kwargs = dict(\n",
    "    cali_data=cali_data,\n",
    "    iters=args.iters_w,\n",
    "    weight=args.weight,\n",
    "    b_range=(args.b_start, args.b_end),\n",
    "    warmup=args.warmup,\n",
    "    opt_mode=\"mse\",\n",
    "    lr=args.lr,\n",
    "    input_prob=args.input_prob,\n",
    "    keep_gpu=not args.keep_cpu,\n",
    "    lamb_r=args.lamb_r,\n",
    "    T=args.T,\n",
    "    bn_lr=args.bn_lr,\n",
    "    lamb_c=args.lamb_c,\n",
    ")\n",
    "\n",
    "\"\"\"init weight quantizer\"\"\"\n",
    "set_weight_quantize_params(qnn)\n",
    "\n",
    "def set_weight_act_quantize_params(module, fp_module):\n",
    "    if isinstance(module, QuantModule):\n",
    "        layer_reconstruction(\n",
    "            qnn, fp_model, module, fp_module, **kwargs, fromCheckPoint=True\n",
    "        )\n",
    "    elif isinstance(module, BaseQuantBlock):\n",
    "        block_reconstruction(\n",
    "            qnn, fp_model, module, fp_module, **kwargs, fromCheckPoint=True\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def recon_model(model: nn.Module, fp_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Block reconstruction. For the first and last layers, we can only apply layer reconstruction.\n",
    "    \"\"\"\n",
    "    for (name, module), (_, fp_module) in zip(\n",
    "        model.named_children(), fp_model.named_children()\n",
    "    ):\n",
    "        if isinstance(module, QuantModule):\n",
    "            print(\"Reconstruction for layer {}\".format(name))\n",
    "            set_weight_act_quantize_params(module, fp_module)\n",
    "        elif isinstance(module, BaseQuantBlock):\n",
    "            print(\"Reconstruction for block {}\".format(name))\n",
    "            set_weight_act_quantize_params(module, fp_module)\n",
    "        else:\n",
    "            recon_model(module, fp_module)\n",
    "\n",
    "# Start calibration\n",
    "recon_model(qnn, fp_model)\n",
    "\n",
    "qnn.load_state_dict(\n",
    "    torch.load(\n",
    "        # f\"logs/W{args.n_bits_w}A{args.n_bits_a}_calib{args.num_samples}_batch{args.batch_size}_iterw{args.iters_w}/{args.arch}/{args.filename}.pth\"\n",
    "        \"logs/W4A4_calib1024_batch64_iterW20000/resnet18/backup/default.pth\"\n",
    "    )\n",
    ")\n",
    "\n",
    "qnn.set_quant_state(weight_quant=True, act_quant=True)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Full quantization (W{}A{}) accuracy: {}\".format(\n",
    "        args.n_bits_w, args.n_bits_a, validate_model(test_loader, qnn)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction for layer conv1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for layer fc True True\n"
     ]
    }
   ],
   "source": [
    "def printModuleQuantOnOff(model: nn.Module, fp_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Block reconstruction. For the first and last layers, we can only apply layer reconstruction.\n",
    "    \"\"\"\n",
    "    for (name, module), (_, fp_module) in zip(\n",
    "        model.named_children(), fp_model.named_children()\n",
    "    ):\n",
    "\n",
    "        if isinstance(module, QuantModule):\n",
    "            print(\n",
    "                \"Reconstruction for layer {}\".format(name),\n",
    "                module.use_weight_quant,\n",
    "                module.use_act_quant,\n",
    "            )\n",
    "        elif isinstance(module, BaseQuantBlock):\n",
    "            print(\n",
    "                \"Reconstruction for block {}\".format(name),\n",
    "                module.use_weight_quant,\n",
    "                module.use_act_quant,\n",
    "            )\n",
    "        else:\n",
    "            printModuleQuantOnOff(module, fp_module)\n",
    "\n",
    "\n",
    "printModuleQuantOnOff(qnn, fp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction for layer conv1\n",
      "Reconstruction for block 0\n",
      "Reconstruction for block 1\n",
      "Reconstruction for block 0\n",
      "Reconstruction for block 1\n",
      "Reconstruction for block 0\n",
      "Reconstruction for block 1\n",
      "Reconstruction for block 0\n",
      "Reconstruction for block 1\n",
      "8\n",
      "Quantization disable for layer fc\n",
      "Reconstruction for layer conv1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 True True\n",
      "Reconstruction for block 0 True True\n",
      "Reconstruction for block 1 False False\n",
      "Reconstruction for layer fc False False\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "def printModuleName(model: nn.Module, fp_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Block reconstruction. For the first and last layers, we can only apply layer reconstruction.\n",
    "    \"\"\"\n",
    "    for (name, module), (_, fp_module) in zip(\n",
    "        model.named_children(), fp_model.named_children()\n",
    "    ):\n",
    "        if name == \"fc\":\n",
    "            print(\"Quantization disable for layer {}\".format(name))\n",
    "            module.set_quant_state(False, False)\n",
    "            break\n",
    "        global cnt\n",
    "        if isinstance(module, QuantModule):\n",
    "            print(\"Reconstruction for layer {}\".format(name))\n",
    "        elif isinstance(module, BaseQuantBlock):\n",
    "            cnt += 1\n",
    "            print(\"Reconstruction for block {}\".format(name))\n",
    "            if cnt == 8:\n",
    "                module.set_quant_state(False, False)\n",
    "                print(cnt)\n",
    "        else:\n",
    "            printModuleName(module, fp_module)\n",
    "\n",
    "\n",
    "printModuleName(qnn, fp_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "printModuleQuantOnOff(qnn, fp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [  0/782]\tTime  6.888 ( 6.888)\tAcc@1  87.50 ( 87.50)\tAcc@5  93.75 ( 93.75)\tInferenceSpeed (ms)  6.888 ( 6.888)\n",
      "Test: [100/782]\tTime  5.240 ( 8.596)\tAcc@1  84.38 ( 76.45)\tAcc@5  96.88 ( 92.48)\tInferenceSpeed (ms)  5.240 ( 8.596)\n",
      "Test: [200/782]\tTime  5.734 ( 8.074)\tAcc@1  73.44 ( 75.86)\tAcc@5  92.19 ( 93.45)\tInferenceSpeed (ms)  5.734 ( 8.074)\n",
      "Test: [300/782]\tTime  5.503 ( 8.003)\tAcc@1  78.12 ( 76.10)\tAcc@5  96.88 ( 93.49)\tInferenceSpeed (ms)  5.503 ( 8.003)\n",
      "Test: [400/782]\tTime  5.570 ( 7.965)\tAcc@1  68.75 ( 73.20)\tAcc@5  96.88 ( 91.75)\tInferenceSpeed (ms)  5.570 ( 7.965)\n",
      "Test: [500/782]\tTime  5.136 ( 8.081)\tAcc@1  75.00 ( 71.64)\tAcc@5  93.75 ( 90.49)\tInferenceSpeed (ms)  5.136 ( 8.081)\n",
      "Test: [600/782]\tTime  5.375 ( 7.994)\tAcc@1  81.25 ( 70.53)\tAcc@5  89.06 ( 89.65)\tInferenceSpeed (ms)  5.375 ( 7.994)\n",
      "Test: [700/782]\tTime  5.584 ( 8.003)\tAcc@1  70.31 ( 69.51)\tAcc@5  90.62 ( 89.00)\tInferenceSpeed (ms)  5.584 ( 8.003)\n",
      " * Acc@1 69.418 Acc@5 89.038 Inference Speed 7.898 ms/batch\n",
      "Full quantization (W4A4) accuracy: 69.41799926757812\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Full quantization (W{}A{}) accuracy: {}\".format(\n",
    "        args.n_bits_w, args.n_bits_a, validate_model(test_loader, qnn)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn.set_quant_state(weight_quant=True, act_quant=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "activations = {\"layer1\": [], \"layer2\": [], \"layer3\": [], \"layer4\": [], \"fc\": []}\n",
    "model = qnn\n",
    "# Forward hook 함수 정의\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name].append(output.detach().cpu())\n",
    "\n",
    "    return hook\n",
    "\n",
    "hooks = []\n",
    "# hooks.append(model.layer4.register_forward_hook(get_activation(\"layer4\")))\n",
    "hooks.append(model.register_forward_hook(get_activation(\"fc\")))\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 몇 개의 배치를 통해 활성화 추출\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i >= 10:  # 예시로 100개의 이미지만 사용\n",
    "            break\n",
    "        _ = model(images.to(\"cuda\"))\n",
    "\n",
    "# Hook 제거\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "\n",
    "# 활성화 배열을 하나로 합침 및 히스토그램 그리기 함수\n",
    "def plot_hist(layer_name, logScale=False):\n",
    "    all_activations = torch.concatenate(activations[layer_name], axis=0)\n",
    "    plt.hist(all_activations.flatten(), bins=100)\n",
    "    plt.title(f\"Activation Distribution in {layer_name}\")\n",
    "    plt.xlabel(\"Activation Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if logScale:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 다른 레이어 활성화 분포 히스토그램 (예시로 layer1)\n",
    "# plot_hist(\"layer1\", logScale=True)\n",
    "# plot_hist(\"layer2\", logScale=True)\n",
    "# plot_hist(\"layer3\", logScale=True)\n",
    "# plot_hist(\"layer4\", logScale=True)\n",
    "# FC 레이어 활성화 분포 히스토그램\n",
    "plot_hist(\"fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
