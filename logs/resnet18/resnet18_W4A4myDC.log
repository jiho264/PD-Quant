==> Using Pytorch Dataset
Setting the first and the last layer to 8-bit
the quantized model is below!
QuantModel(
  (model): ResNet(
    (conv1): QuantModule(
      wbit=8, abit=4, disable_act_quant=False
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): ReLU(inplace=True)
    )
    (bn1): StraightThrough()
    (relu): StraightThrough()
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer2): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer3): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer4): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QuantModule(
      wbit=8, abit=4, disable_act_quant=True
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): StraightThrough()
    )
  )
)
Reconstruction for layer conv1
Start correcting 32 batches of data!
Init alpha to be FP32
Total loss:	31.336 (rec:31.324, pd:0.012, round:0.000)	b=0.00	count=500
Total loss:	86.873 (rec:86.831, pd:0.042, round:0.000)	b=0.00	count=1000
Total loss:	54.972 (rec:54.926, pd:0.046, round:0.000)	b=0.00	count=1500
Total loss:	73.790 (rec:73.733, pd:0.057, round:0.000)	b=0.00	count=2000
Total loss:	47.270 (rec:47.221, pd:0.048, round:0.000)	b=0.00	count=2500
Total loss:	99.369 (rec:99.319, pd:0.050, round:0.000)	b=0.00	count=3000
Total loss:	71.229 (rec:71.185, pd:0.044, round:0.000)	b=0.00	count=3500
Total loss:	212.072 (rec:187.946, pd:0.051, round:24.074)	b=20.00	count=4000
Total loss:	68.918 (rec:59.720, pd:0.051, round:9.147)	b=19.44	count=4500
Total loss:	99.386 (rec:90.981, pd:0.047, round:8.358)	b=18.88	count=5000
Total loss:	86.980 (rec:79.314, pd:0.054, round:7.612)	b=18.31	count=5500
Total loss:	44.155 (rec:37.287, pd:0.049, round:6.820)	b=17.75	count=6000
Total loss:	73.411 (rec:67.114, pd:0.049, round:6.248)	b=17.19	count=6500
Total loss:	45.213 (rec:39.380, pd:0.043, round:5.791)	b=16.62	count=7000
Total loss:	219.398 (rec:213.900, pd:0.050, round:5.449)	b=16.06	count=7500
Total loss:	71.663 (rec:66.644, pd:0.047, round:4.972)	b=15.50	count=8000
Total loss:	68.665 (rec:64.143, pd:0.047, round:4.475)	b=14.94	count=8500
Total loss:	94.210 (rec:90.131, pd:0.039, round:4.041)	b=14.38	count=9000
Total loss:	102.572 (rec:98.822, pd:0.048, round:3.702)	b=13.81	count=9500
Total loss:	75.581 (rec:72.102, pd:0.047, round:3.431)	b=13.25	count=10000
Total loss:	40.693 (rec:37.411, pd:0.045, round:3.236)	b=12.69	count=10500
Total loss:	49.005 (rec:46.027, pd:0.041, round:2.936)	b=12.12	count=11000
Total loss:	33.918 (rec:31.189, pd:0.051, round:2.679)	b=11.56	count=11500
Total loss:	83.394 (rec:81.065, pd:0.043, round:2.286)	b=11.00	count=12000
Total loss:	42.780 (rec:40.640, pd:0.055, round:2.085)	b=10.44	count=12500
Total loss:	38.569 (rec:36.691, pd:0.046, round:1.832)	b=9.88	count=13000
Total loss:	55.516 (rec:53.758, pd:0.049, round:1.710)	b=9.31	count=13500
Total loss:	95.527 (rec:94.002, pd:0.045, round:1.481)	b=8.75	count=14000
Total loss:	57.012 (rec:55.688, pd:0.043, round:1.281)	b=8.19	count=14500
Total loss:	72.037 (rec:70.894, pd:0.042, round:1.101)	b=7.62	count=15000
Total loss:	79.217 (rec:78.316, pd:0.049, round:0.851)	b=7.06	count=15500
Total loss:	68.039 (rec:67.337, pd:0.060, round:0.642)	b=6.50	count=16000
Total loss:	97.371 (rec:96.820, pd:0.041, round:0.510)	b=5.94	count=16500
Total loss:	87.202 (rec:86.760, pd:0.051, round:0.391)	b=5.38	count=17000
Total loss:	58.940 (rec:58.667, pd:0.043, round:0.230)	b=4.81	count=17500
Total loss:	115.543 (rec:115.384, pd:0.041, round:0.118)	b=4.25	count=18000
Total loss:	168.397 (rec:168.313, pd:0.044, round:0.041)	b=3.69	count=18500
Total loss:	61.049 (rec:60.991, pd:0.048, round:0.010)	b=3.12	count=19000
Total loss:	50.927 (rec:50.883, pd:0.044, round:0.000)	b=2.56	count=19500
Total loss:	182.081 (rec:182.024, pd:0.057, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	4.996 (rec:4.646, pd:0.350, round:0.000)	b=0.00	count=500
Total loss:	4.521 (rec:4.274, pd:0.247, round:0.000)	b=0.00	count=1000
Total loss:	6.279 (rec:6.024, pd:0.256, round:0.000)	b=0.00	count=1500
Total loss:	2.644 (rec:2.432, pd:0.212, round:0.000)	b=0.00	count=2000
Total loss:	3.504 (rec:3.244, pd:0.260, round:0.000)	b=0.00	count=2500
Total loss:	3.088 (rec:2.895, pd:0.193, round:0.000)	b=0.00	count=3000
Total loss:	3.126 (rec:2.933, pd:0.193, round:0.000)	b=0.00	count=3500
Total loss:	527.473 (rec:2.379, pd:0.188, round:524.906)	b=20.00	count=4000
Total loss:	287.962 (rec:2.599, pd:0.209, round:285.155)	b=19.44	count=4500
Total loss:	264.405 (rec:3.234, pd:0.175, round:260.996)	b=18.88	count=5000
Total loss:	245.236 (rec:2.618, pd:0.203, round:242.414)	b=18.31	count=5500
Total loss:	233.099 (rec:6.504, pd:0.167, round:226.428)	b=17.75	count=6000
Total loss:	214.529 (rec:2.836, pd:0.206, round:211.487)	b=17.19	count=6500
Total loss:	200.691 (rec:2.333, pd:0.149, round:198.209)	b=16.62	count=7000
Total loss:	188.379 (rec:2.419, pd:0.184, round:185.776)	b=16.06	count=7500
Total loss:	177.277 (rec:2.309, pd:0.169, round:174.799)	b=15.50	count=8000
Total loss:	167.660 (rec:3.249, pd:0.164, round:164.247)	b=14.94	count=8500
Total loss:	157.038 (rec:2.814, pd:0.194, round:154.030)	b=14.38	count=9000
Total loss:	146.242 (rec:2.251, pd:0.166, round:143.825)	b=13.81	count=9500
Total loss:	136.813 (rec:2.678, pd:0.184, round:133.951)	b=13.25	count=10000
Total loss:	128.066 (rec:2.760, pd:0.215, round:125.091)	b=12.69	count=10500
Total loss:	119.595 (rec:3.117, pd:0.199, round:116.279)	b=12.12	count=11000
Total loss:	110.667 (rec:2.616, pd:0.201, round:107.850)	b=11.56	count=11500
Total loss:	103.754 (rec:3.418, pd:0.150, round:100.186)	b=11.00	count=12000
Total loss:	98.327 (rec:5.578, pd:0.195, round:92.554)	b=10.44	count=12500
Total loss:	87.538 (rec:2.580, pd:0.203, round:84.755)	b=9.88	count=13000
Total loss:	81.605 (rec:4.195, pd:0.151, round:77.259)	b=9.31	count=13500
Total loss:	72.189 (rec:2.250, pd:0.159, round:69.780)	b=8.75	count=14000
Total loss:	68.063 (rec:5.545, pd:0.159, round:62.358)	b=8.19	count=14500
Total loss:	57.155 (rec:2.492, pd:0.172, round:54.492)	b=7.62	count=15000
Total loss:	50.697 (rec:2.944, pd:0.189, round:47.564)	b=7.06	count=15500
Total loss:	43.187 (rec:2.554, pd:0.190, round:40.444)	b=6.50	count=16000
Total loss:	35.524 (rec:2.333, pd:0.167, round:33.025)	b=5.94	count=16500
Total loss:	28.920 (rec:3.134, pd:0.195, round:25.591)	b=5.38	count=17000
Total loss:	21.073 (rec:2.669, pd:0.165, round:18.239)	b=4.81	count=17500
Total loss:	14.330 (rec:2.495, pd:0.175, round:11.660)	b=4.25	count=18000
Total loss:	8.667 (rec:2.997, pd:0.160, round:5.511)	b=3.69	count=18500
Total loss:	4.054 (rec:2.940, pd:0.190, round:0.923)	b=3.12	count=19000
Total loss:	2.879 (rec:2.654, pd:0.205, round:0.020)	b=2.56	count=19500
Total loss:	3.494 (rec:3.316, pd:0.178, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	2.965 (rec:2.778, pd:0.187, round:0.000)	b=0.00	count=500
Total loss:	3.062 (rec:2.892, pd:0.171, round:0.000)	b=0.00	count=1000
Total loss:	2.356 (rec:2.200, pd:0.156, round:0.000)	b=0.00	count=1500
Total loss:	2.107 (rec:1.938, pd:0.169, round:0.000)	b=0.00	count=2000
Total loss:	2.102 (rec:1.951, pd:0.151, round:0.000)	b=0.00	count=2500
Total loss:	2.071 (rec:1.937, pd:0.135, round:0.000)	b=0.00	count=3000
Total loss:	2.437 (rec:2.263, pd:0.174, round:0.000)	b=0.00	count=3500
Total loss:	581.457 (rec:2.022, pd:0.177, round:579.257)	b=20.00	count=4000
Total loss:	322.862 (rec:2.868, pd:0.127, round:319.866)	b=19.44	count=4500
Total loss:	297.687 (rec:1.840, pd:0.148, round:295.700)	b=18.88	count=5000
Total loss:	280.161 (rec:1.975, pd:0.147, round:278.039)	b=18.31	count=5500
Total loss:	264.124 (rec:2.161, pd:0.141, round:261.822)	b=17.75	count=6000
Total loss:	249.792 (rec:2.034, pd:0.127, round:247.631)	b=17.19	count=6500
Total loss:	236.954 (rec:1.971, pd:0.148, round:234.835)	b=16.62	count=7000
Total loss:	223.997 (rec:1.810, pd:0.153, round:222.034)	b=16.06	count=7500
Total loss:	212.941 (rec:2.708, pd:0.139, round:210.094)	b=15.50	count=8000
Total loss:	200.599 (rec:2.239, pd:0.138, round:198.221)	b=14.94	count=8500
Total loss:	189.669 (rec:2.185, pd:0.167, round:187.316)	b=14.38	count=9000
Total loss:	179.811 (rec:3.046, pd:0.134, round:176.631)	b=13.81	count=9500
Total loss:	168.294 (rec:1.902, pd:0.128, round:166.264)	b=13.25	count=10000
Total loss:	158.271 (rec:2.025, pd:0.157, round:156.089)	b=12.69	count=10500
Total loss:	147.641 (rec:1.710, pd:0.133, round:145.798)	b=12.12	count=11000
Total loss:	137.871 (rec:2.194, pd:0.123, round:135.554)	b=11.56	count=11500
Total loss:	128.361 (rec:3.065, pd:0.141, round:125.155)	b=11.00	count=12000
Total loss:	116.489 (rec:1.749, pd:0.129, round:114.611)	b=10.44	count=12500
Total loss:	106.958 (rec:2.198, pd:0.129, round:104.631)	b=9.88	count=13000
Total loss:	96.436 (rec:1.810, pd:0.124, round:94.502)	b=9.31	count=13500
Total loss:	86.204 (rec:2.024, pd:0.137, round:84.044)	b=8.75	count=14000
Total loss:	76.842 (rec:2.032, pd:0.133, round:74.676)	b=8.19	count=14500
Total loss:	66.878 (rec:2.327, pd:0.100, round:64.451)	b=7.62	count=15000
Total loss:	57.403 (rec:2.345, pd:0.123, round:54.935)	b=7.06	count=15500
Total loss:	46.920 (rec:1.783, pd:0.132, round:45.004)	b=6.50	count=16000
Total loss:	37.469 (rec:2.106, pd:0.141, round:35.222)	b=5.94	count=16500
Total loss:	27.910 (rec:2.144, pd:0.149, round:25.617)	b=5.38	count=17000
Total loss:	19.141 (rec:2.463, pd:0.133, round:16.545)	b=4.81	count=17500
Total loss:	10.454 (rec:2.481, pd:0.107, round:7.866)	b=4.25	count=18000
Total loss:	3.849 (rec:1.757, pd:0.142, round:1.950)	b=3.69	count=18500
Total loss:	2.988 (rec:2.657, pd:0.133, round:0.198)	b=3.12	count=19000
Total loss:	2.277 (rec:2.151, pd:0.117, round:0.008)	b=2.56	count=19500
Total loss:	2.012 (rec:1.891, pd:0.121, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	1.803 (rec:1.632, pd:0.170, round:0.000)	b=0.00	count=500
Total loss:	1.594 (rec:1.463, pd:0.131, round:0.000)	b=0.00	count=1000
Total loss:	1.913 (rec:1.760, pd:0.152, round:0.000)	b=0.00	count=1500
Total loss:	2.174 (rec:2.028, pd:0.146, round:0.000)	b=0.00	count=2000
Total loss:	1.538 (rec:1.406, pd:0.132, round:0.000)	b=0.00	count=2500
Total loss:	1.713 (rec:1.579, pd:0.134, round:0.000)	b=0.00	count=3000
Total loss:	1.536 (rec:1.391, pd:0.145, round:0.000)	b=0.00	count=3500
Total loss:	1955.773 (rec:1.294, pd:0.120, round:1954.359)	b=20.00	count=4000
Total loss:	1053.649 (rec:1.225, pd:0.135, round:1052.290)	b=19.44	count=4500
Total loss:	979.921 (rec:1.593, pd:0.123, round:978.205)	b=18.88	count=5000
Total loss:	925.353 (rec:2.536, pd:0.125, round:922.691)	b=18.31	count=5500
Total loss:	876.384 (rec:1.462, pd:0.129, round:874.793)	b=17.75	count=6000
Total loss:	831.556 (rec:1.494, pd:0.136, round:829.926)	b=17.19	count=6500
Total loss:	788.445 (rec:1.416, pd:0.142, round:786.887)	b=16.62	count=7000
Total loss:	746.562 (rec:1.220, pd:0.116, round:745.225)	b=16.06	count=7500
Total loss:	706.544 (rec:1.342, pd:0.111, round:705.090)	b=15.50	count=8000
Total loss:	668.419 (rec:1.960, pd:0.137, round:666.322)	b=14.94	count=8500
Total loss:	629.485 (rec:1.250, pd:0.115, round:628.120)	b=14.38	count=9000
Total loss:	592.433 (rec:1.501, pd:0.123, round:590.809)	b=13.81	count=9500
Total loss:	556.131 (rec:1.727, pd:0.126, round:554.278)	b=13.25	count=10000
Total loss:	519.997 (rec:1.266, pd:0.121, round:518.610)	b=12.69	count=10500
Total loss:	484.331 (rec:1.431, pd:0.136, round:482.764)	b=12.12	count=11000
Total loss:	448.498 (rec:1.213, pd:0.115, round:447.170)	b=11.56	count=11500
Total loss:	411.968 (rec:1.244, pd:0.133, round:410.591)	b=11.00	count=12000
Total loss:	376.203 (rec:1.560, pd:0.134, round:374.509)	b=10.44	count=12500
Total loss:	339.667 (rec:1.350, pd:0.120, round:338.197)	b=9.88	count=13000
Total loss:	303.871 (rec:1.340, pd:0.126, round:302.405)	b=9.31	count=13500
Total loss:	267.780 (rec:1.353, pd:0.114, round:266.313)	b=8.75	count=14000
Total loss:	233.528 (rec:1.239, pd:0.107, round:232.181)	b=8.19	count=14500
Total loss:	198.710 (rec:1.314, pd:0.128, round:197.267)	b=7.62	count=15000
Total loss:	163.565 (rec:1.532, pd:0.112, round:161.922)	b=7.06	count=15500
Total loss:	128.825 (rec:1.513, pd:0.125, round:127.186)	b=6.50	count=16000
Total loss:	94.018 (rec:1.186, pd:0.108, round:92.725)	b=5.94	count=16500
Total loss:	60.888 (rec:1.417, pd:0.113, round:59.357)	b=5.38	count=17000
Total loss:	30.373 (rec:1.659, pd:0.115, round:28.599)	b=4.81	count=17500
Total loss:	8.013 (rec:1.188, pd:0.119, round:6.706)	b=4.25	count=18000
Total loss:	2.324 (rec:1.423, pd:0.134, round:0.767)	b=3.69	count=18500
Total loss:	1.541 (rec:1.385, pd:0.146, round:0.010)	b=3.12	count=19000
Total loss:	1.383 (rec:1.249, pd:0.134, round:0.000)	b=2.56	count=19500
Total loss:	1.431 (rec:1.306, pd:0.125, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	1.172 (rec:1.083, pd:0.089, round:0.000)	b=0.00	count=500
Total loss:	1.122 (rec:1.041, pd:0.081, round:0.000)	b=0.00	count=1000
Total loss:	1.224 (rec:1.134, pd:0.090, round:0.000)	b=0.00	count=1500
Total loss:	1.073 (rec:0.993, pd:0.079, round:0.000)	b=0.00	count=2000
Total loss:	1.021 (rec:0.936, pd:0.084, round:0.000)	b=0.00	count=2500
Total loss:	1.154 (rec:1.076, pd:0.078, round:0.000)	b=0.00	count=3000
Total loss:	1.131 (rec:1.041, pd:0.091, round:0.000)	b=0.00	count=3500
Total loss:	2510.214 (rec:1.058, pd:0.081, round:2509.076)	b=20.00	count=4000
Total loss:	1279.207 (rec:1.017, pd:0.073, round:1278.118)	b=19.44	count=4500
Total loss:	1185.457 (rec:0.959, pd:0.079, round:1184.419)	b=18.88	count=5000
Total loss:	1116.235 (rec:1.050, pd:0.069, round:1115.116)	b=18.31	count=5500
Total loss:	1054.150 (rec:1.013, pd:0.073, round:1053.064)	b=17.75	count=6000
Total loss:	997.697 (rec:0.993, pd:0.079, round:996.625)	b=17.19	count=6500
Total loss:	944.459 (rec:0.961, pd:0.076, round:943.422)	b=16.62	count=7000
Total loss:	894.483 (rec:1.123, pd:0.069, round:893.291)	b=16.06	count=7500
Total loss:	844.984 (rec:1.032, pd:0.079, round:843.873)	b=15.50	count=8000
Total loss:	795.882 (rec:0.994, pd:0.080, round:794.808)	b=14.94	count=8500
Total loss:	748.617 (rec:0.963, pd:0.078, round:747.576)	b=14.38	count=9000
Total loss:	703.239 (rec:1.100, pd:0.078, round:702.061)	b=13.81	count=9500
Total loss:	655.895 (rec:1.061, pd:0.072, round:654.762)	b=13.25	count=10000
Total loss:	611.121 (rec:1.129, pd:0.079, round:609.912)	b=12.69	count=10500
Total loss:	566.698 (rec:1.001, pd:0.065, round:565.632)	b=12.12	count=11000
Total loss:	522.095 (rec:1.044, pd:0.070, round:520.981)	b=11.56	count=11500
Total loss:	476.665 (rec:1.078, pd:0.080, round:475.508)	b=11.00	count=12000
Total loss:	432.267 (rec:1.074, pd:0.075, round:431.117)	b=10.44	count=12500
Total loss:	387.922 (rec:1.107, pd:0.075, round:386.740)	b=9.88	count=13000
Total loss:	342.525 (rec:1.155, pd:0.075, round:341.295)	b=9.31	count=13500
Total loss:	298.074 (rec:1.145, pd:0.068, round:296.861)	b=8.75	count=14000
Total loss:	252.980 (rec:1.198, pd:0.080, round:251.701)	b=8.19	count=14500
Total loss:	208.218 (rec:0.958, pd:0.068, round:207.191)	b=7.62	count=15000
Total loss:	164.240 (rec:1.020, pd:0.072, round:163.148)	b=7.06	count=15500
Total loss:	120.862 (rec:1.011, pd:0.072, round:119.779)	b=6.50	count=16000
Total loss:	79.097 (rec:1.023, pd:0.081, round:77.993)	b=5.94	count=16500
Total loss:	40.111 (rec:0.967, pd:0.066, round:39.079)	b=5.38	count=17000
Total loss:	9.972 (rec:1.023, pd:0.073, round:8.876)	b=4.81	count=17500
Total loss:	1.852 (rec:1.044, pd:0.067, round:0.741)	b=4.25	count=18000
Total loss:	1.258 (rec:1.132, pd:0.089, round:0.037)	b=3.69	count=18500
Total loss:	1.069 (rec:1.000, pd:0.069, round:0.000)	b=3.12	count=19000
Total loss:	1.174 (rec:1.101, pd:0.072, round:0.000)	b=2.56	count=19500
Total loss:	1.058 (rec:0.976, pd:0.082, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.850 (rec:0.781, pd:0.070, round:0.000)	b=0.00	count=500
Total loss:	0.827 (rec:0.759, pd:0.069, round:0.000)	b=0.00	count=1000
Total loss:	0.761 (rec:0.696, pd:0.065, round:0.000)	b=0.00	count=1500
Total loss:	0.772 (rec:0.714, pd:0.059, round:0.000)	b=0.00	count=2000
Total loss:	0.791 (rec:0.734, pd:0.058, round:0.000)	b=0.00	count=2500
Total loss:	0.831 (rec:0.775, pd:0.056, round:0.000)	b=0.00	count=3000
Total loss:	0.744 (rec:0.691, pd:0.053, round:0.000)	b=0.00	count=3500
Total loss:	8023.382 (rec:0.710, pd:0.052, round:8022.620)	b=20.00	count=4000
Total loss:	3941.625 (rec:0.605, pd:0.049, round:3940.970)	b=19.44	count=4500
Total loss:	3652.256 (rec:0.624, pd:0.052, round:3651.581)	b=18.88	count=5000
Total loss:	3433.825 (rec:0.799, pd:0.052, round:3432.974)	b=18.31	count=5500
Total loss:	3241.679 (rec:0.646, pd:0.053, round:3240.980)	b=17.75	count=6000
Total loss:	3060.025 (rec:0.691, pd:0.052, round:3059.281)	b=17.19	count=6500
Total loss:	2886.166 (rec:0.653, pd:0.049, round:2885.463)	b=16.62	count=7000
Total loss:	2720.855 (rec:0.663, pd:0.050, round:2720.143)	b=16.06	count=7500
Total loss:	2559.198 (rec:0.813, pd:0.053, round:2558.332)	b=15.50	count=8000
Total loss:	2399.205 (rec:0.676, pd:0.052, round:2398.478)	b=14.94	count=8500
Total loss:	2242.904 (rec:0.765, pd:0.045, round:2242.094)	b=14.38	count=9000
Total loss:	2092.561 (rec:0.655, pd:0.042, round:2091.864)	b=13.81	count=9500
Total loss:	1941.211 (rec:0.664, pd:0.048, round:1940.498)	b=13.25	count=10000
Total loss:	1792.122 (rec:0.733, pd:0.052, round:1791.338)	b=12.69	count=10500
Total loss:	1645.669 (rec:0.795, pd:0.045, round:1644.828)	b=12.12	count=11000
Total loss:	1501.373 (rec:0.701, pd:0.045, round:1500.627)	b=11.56	count=11500
Total loss:	1357.952 (rec:0.776, pd:0.046, round:1357.130)	b=11.00	count=12000
Total loss:	1212.973 (rec:0.753, pd:0.045, round:1212.175)	b=10.44	count=12500
Total loss:	1071.124 (rec:0.666, pd:0.046, round:1070.412)	b=9.88	count=13000
Total loss:	932.770 (rec:0.849, pd:0.049, round:931.873)	b=9.31	count=13500
Total loss:	795.298 (rec:0.738, pd:0.048, round:794.513)	b=8.75	count=14000
Total loss:	658.938 (rec:0.768, pd:0.043, round:658.126)	b=8.19	count=14500
Total loss:	526.047 (rec:0.665, pd:0.048, round:525.334)	b=7.62	count=15000
Total loss:	395.872 (rec:0.726, pd:0.047, round:395.100)	b=7.06	count=15500
Total loss:	270.126 (rec:0.651, pd:0.044, round:269.431)	b=6.50	count=16000
Total loss:	152.508 (rec:0.741, pd:0.041, round:151.726)	b=5.94	count=16500
Total loss:	48.658 (rec:0.682, pd:0.046, round:47.930)	b=5.38	count=17000
Total loss:	4.875 (rec:0.677, pd:0.046, round:4.151)	b=4.81	count=17500
Total loss:	1.106 (rec:0.649, pd:0.051, round:0.406)	b=4.25	count=18000
Total loss:	0.943 (rec:0.827, pd:0.049, round:0.067)	b=3.69	count=18500
Total loss:	0.812 (rec:0.766, pd:0.045, round:0.000)	b=3.12	count=19000
Total loss:	0.741 (rec:0.698, pd:0.043, round:0.000)	b=2.56	count=19500
Total loss:	0.751 (rec:0.703, pd:0.048, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	1.151 (rec:1.093, pd:0.058, round:0.000)	b=0.00	count=500
Total loss:	1.315 (rec:1.262, pd:0.053, round:0.000)	b=0.00	count=1000
Total loss:	1.199 (rec:1.153, pd:0.046, round:0.000)	b=0.00	count=1500
Total loss:	1.138 (rec:1.093, pd:0.046, round:0.000)	b=0.00	count=2000
Total loss:	1.206 (rec:1.157, pd:0.049, round:0.000)	b=0.00	count=2500
Total loss:	1.132 (rec:1.085, pd:0.047, round:0.000)	b=0.00	count=3000
Total loss:	1.151 (rec:1.110, pd:0.041, round:0.000)	b=0.00	count=3500
Total loss:	10059.937 (rec:1.004, pd:0.042, round:10058.891)	b=20.00	count=4000
Total loss:	4741.075 (rec:0.961, pd:0.040, round:4740.074)	b=19.44	count=4500
Total loss:	4370.865 (rec:0.993, pd:0.042, round:4369.831)	b=18.88	count=5000
Total loss:	4081.779 (rec:1.187, pd:0.044, round:4080.547)	b=18.31	count=5500
Total loss:	3825.810 (rec:0.995, pd:0.042, round:3824.773)	b=17.75	count=6000
Total loss:	3585.390 (rec:1.291, pd:0.047, round:3584.051)	b=17.19	count=6500
Total loss:	3357.759 (rec:0.989, pd:0.040, round:3356.730)	b=16.62	count=7000
Total loss:	3139.840 (rec:1.137, pd:0.041, round:3138.661)	b=16.06	count=7500
Total loss:	2929.665 (rec:1.191, pd:0.038, round:2928.435)	b=15.50	count=8000
Total loss:	2723.062 (rec:1.025, pd:0.037, round:2722.001)	b=14.94	count=8500
Total loss:	2527.707 (rec:1.096, pd:0.039, round:2526.572)	b=14.38	count=9000
Total loss:	2333.726 (rec:0.967, pd:0.039, round:2332.721)	b=13.81	count=9500
Total loss:	2148.503 (rec:0.999, pd:0.036, round:2147.468)	b=13.25	count=10000
Total loss:	1967.628 (rec:1.045, pd:0.039, round:1966.545)	b=12.69	count=10500
Total loss:	1790.457 (rec:1.094, pd:0.041, round:1789.322)	b=12.12	count=11000
Total loss:	1618.625 (rec:1.062, pd:0.037, round:1617.526)	b=11.56	count=11500
Total loss:	1454.151 (rec:0.988, pd:0.041, round:1453.122)	b=11.00	count=12000
Total loss:	1294.989 (rec:1.059, pd:0.039, round:1293.890)	b=10.44	count=12500
Total loss:	1133.021 (rec:1.103, pd:0.039, round:1131.879)	b=9.88	count=13000
Total loss:	980.796 (rec:0.984, pd:0.038, round:979.774)	b=9.31	count=13500
Total loss:	830.638 (rec:1.137, pd:0.040, round:829.461)	b=8.75	count=14000
Total loss:	684.540 (rec:1.118, pd:0.040, round:683.383)	b=8.19	count=14500
Total loss:	540.687 (rec:1.201, pd:0.044, round:539.441)	b=7.62	count=15000
Total loss:	403.146 (rec:1.123, pd:0.042, round:401.981)	b=7.06	count=15500
Total loss:	274.041 (rec:1.083, pd:0.041, round:272.917)	b=6.50	count=16000
Total loss:	155.357 (rec:1.071, pd:0.042, round:154.244)	b=5.94	count=16500
Total loss:	58.606 (rec:1.132, pd:0.042, round:57.432)	b=5.38	count=17000
Total loss:	11.768 (rec:1.107, pd:0.042, round:10.619)	b=4.81	count=17500
Total loss:	2.091 (rec:1.066, pd:0.045, round:0.980)	b=4.25	count=18000
Total loss:	1.114 (rec:1.019, pd:0.041, round:0.054)	b=3.69	count=18500
Total loss:	1.147 (rec:1.106, pd:0.041, round:0.000)	b=3.12	count=19000
Total loss:	1.166 (rec:1.128, pd:0.039, round:0.000)	b=2.56	count=19500
Total loss:	1.039 (rec:1.002, pd:0.038, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	1.558 (rec:1.513, pd:0.044, round:0.000)	b=0.00	count=500
Total loss:	1.490 (rec:1.448, pd:0.042, round:0.000)	b=0.00	count=1000
Total loss:	1.520 (rec:1.477, pd:0.043, round:0.000)	b=0.00	count=1500
Total loss:	1.501 (rec:1.455, pd:0.046, round:0.000)	b=0.00	count=2000
Total loss:	1.498 (rec:1.456, pd:0.042, round:0.000)	b=0.00	count=2500
Total loss:	1.484 (rec:1.446, pd:0.038, round:0.000)	b=0.00	count=3000
Total loss:	1.360 (rec:1.322, pd:0.038, round:0.000)	b=0.00	count=3500
Total loss:	31126.824 (rec:1.318, pd:0.037, round:31125.469)	b=20.00	count=4000
Total loss:	14291.839 (rec:1.308, pd:0.035, round:14290.496)	b=19.44	count=4500
Total loss:	13021.643 (rec:1.277, pd:0.037, round:13020.328)	b=18.88	count=5000
Total loss:	11997.483 (rec:1.156, pd:0.030, round:11996.297)	b=18.31	count=5500
Total loss:	11081.543 (rec:1.307, pd:0.033, round:11080.203)	b=17.75	count=6000
Total loss:	10227.697 (rec:1.239, pd:0.033, round:10226.426)	b=17.19	count=6500
Total loss:	9429.066 (rec:1.249, pd:0.030, round:9427.787)	b=16.62	count=7000
Total loss:	8672.854 (rec:1.385, pd:0.033, round:8671.437)	b=16.06	count=7500
Total loss:	7955.123 (rec:1.376, pd:0.030, round:7953.716)	b=15.50	count=8000
Total loss:	7279.260 (rec:1.203, pd:0.031, round:7278.025)	b=14.94	count=8500
Total loss:	6632.841 (rec:1.326, pd:0.032, round:6631.483)	b=14.38	count=9000
Total loss:	6026.718 (rec:1.349, pd:0.037, round:6025.332)	b=13.81	count=9500
Total loss:	5455.249 (rec:1.299, pd:0.034, round:5453.916)	b=13.25	count=10000
Total loss:	4915.640 (rec:1.295, pd:0.030, round:4914.315)	b=12.69	count=10500
Total loss:	4406.557 (rec:1.251, pd:0.030, round:4405.276)	b=12.12	count=11000
Total loss:	3924.522 (rec:1.363, pd:0.037, round:3923.122)	b=11.56	count=11500
Total loss:	3465.914 (rec:1.388, pd:0.033, round:3464.492)	b=11.00	count=12000
Total loss:	3032.383 (rec:1.185, pd:0.029, round:3031.168)	b=10.44	count=12500
Total loss:	2624.549 (rec:1.240, pd:0.031, round:2623.278)	b=9.88	count=13000
Total loss:	2238.693 (rec:1.273, pd:0.032, round:2237.388)	b=9.31	count=13500
Total loss:	1870.698 (rec:1.245, pd:0.031, round:1869.422)	b=8.75	count=14000
Total loss:	1517.418 (rec:1.243, pd:0.031, round:1516.145)	b=8.19	count=14500
Total loss:	1186.736 (rec:1.168, pd:0.031, round:1185.537)	b=7.62	count=15000
Total loss:	878.598 (rec:1.248, pd:0.031, round:877.319)	b=7.06	count=15500
Total loss:	588.792 (rec:1.304, pd:0.034, round:587.455)	b=6.50	count=16000
Total loss:	336.589 (rec:1.151, pd:0.030, round:335.408)	b=5.94	count=16500
Total loss:	135.425 (rec:1.330, pd:0.032, round:134.063)	b=5.38	count=17000
Total loss:	29.349 (rec:1.203, pd:0.030, round:28.117)	b=4.81	count=17500
Total loss:	4.442 (rec:1.300, pd:0.032, round:3.110)	b=4.25	count=18000
Total loss:	1.254 (rec:1.072, pd:0.029, round:0.154)	b=3.69	count=18500
Total loss:	1.265 (rec:1.233, pd:0.032, round:0.000)	b=3.12	count=19000
Total loss:	1.357 (rec:1.324, pd:0.033, round:0.000)	b=2.56	count=19500
Total loss:	1.417 (rec:1.385, pd:0.032, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	232.009 (rec:231.962, pd:0.047, round:0.000)	b=0.00	count=500
Total loss:	214.974 (rec:214.918, pd:0.057, round:0.000)	b=0.00	count=1000
Total loss:	207.832 (rec:207.773, pd:0.059, round:0.000)	b=0.00	count=1500
Total loss:	185.243 (rec:185.182, pd:0.061, round:0.000)	b=0.00	count=2000
Total loss:	187.834 (rec:187.773, pd:0.061, round:0.000)	b=0.00	count=2500
Total loss:	186.458 (rec:186.404, pd:0.055, round:0.000)	b=0.00	count=3000
Total loss:	178.979 (rec:178.923, pd:0.056, round:0.000)	b=0.00	count=3500
Total loss:	40820.824 (rec:179.426, pd:0.065, round:40641.332)	b=20.00	count=4000
Total loss:	25672.617 (rec:171.416, pd:0.060, round:25501.141)	b=19.44	count=4500
Total loss:	23846.154 (rec:165.467, pd:0.071, round:23680.615)	b=18.88	count=5000
Total loss:	22484.803 (rec:169.724, pd:0.084, round:22314.994)	b=18.31	count=5500
Total loss:	21272.672 (rec:162.060, pd:0.071, round:21110.541)	b=17.75	count=6000
Total loss:	20163.932 (rec:172.218, pd:0.073, round:19991.641)	b=17.19	count=6500
Total loss:	19089.650 (rec:161.984, pd:0.080, round:18927.586)	b=16.62	count=7000
Total loss:	18078.375 (rec:163.578, pd:0.067, round:17914.730)	b=16.06	count=7500
Total loss:	17107.225 (rec:163.779, pd:0.089, round:16943.355)	b=15.50	count=8000
Total loss:	16165.916 (rec:157.026, pd:0.074, round:16008.815)	b=14.94	count=8500
Total loss:	15278.054 (rec:172.838, pd:0.072, round:15105.145)	b=14.38	count=9000
Total loss:	14395.458 (rec:171.977, pd:0.071, round:14223.410)	b=13.81	count=9500
Total loss:	13535.761 (rec:159.435, pd:0.079, round:13376.248)	b=13.25	count=10000
Total loss:	12714.694 (rec:163.001, pd:0.083, round:12551.610)	b=12.69	count=10500
Total loss:	11902.259 (rec:154.940, pd:0.085, round:11747.234)	b=12.12	count=11000
Total loss:	11107.062 (rec:153.336, pd:0.069, round:10953.656)	b=11.56	count=11500
Total loss:	10349.489 (rec:170.245, pd:0.088, round:10179.157)	b=11.00	count=12000
Total loss:	9575.638 (rec:154.293, pd:0.086, round:9421.259)	b=10.44	count=12500
Total loss:	8825.431 (rec:152.845, pd:0.081, round:8672.504)	b=9.88	count=13000
Total loss:	8097.025 (rec:157.513, pd:0.079, round:7939.434)	b=9.31	count=13500
Total loss:	7364.508 (rec:156.235, pd:0.061, round:7208.212)	b=8.75	count=14000
Total loss:	6634.177 (rec:151.040, pd:0.063, round:6483.074)	b=8.19	count=14500
Total loss:	5919.354 (rec:157.648, pd:0.066, round:5761.641)	b=7.62	count=15000
Total loss:	5202.048 (rec:162.333, pd:0.081, round:5039.633)	b=7.06	count=15500
Total loss:	4471.888 (rec:159.209, pd:0.086, round:4312.593)	b=6.50	count=16000
Total loss:	3740.656 (rec:154.250, pd:0.065, round:3586.341)	b=5.94	count=16500
Total loss:	3009.953 (rec:154.638, pd:0.066, round:2855.250)	b=5.38	count=17000
Total loss:	2286.128 (rec:154.923, pd:0.094, round:2131.111)	b=4.81	count=17500
Total loss:	1579.375 (rec:159.113, pd:0.075, round:1420.187)	b=4.25	count=18000
Total loss:	923.496 (rec:167.340, pd:0.073, round:756.083)	b=3.69	count=18500
Total loss:	373.663 (rec:145.730, pd:0.079, round:227.854)	b=3.12	count=19000
Total loss:	177.048 (rec:146.713, pd:0.069, round:30.265)	b=2.56	count=19500
Total loss:	157.331 (rec:149.059, pd:0.080, round:8.193)	b=2.00	count=20000
Reconstruction for layer fc
Start correcting 32 batches of data!
