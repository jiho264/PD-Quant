==> Using Pytorch Dataset
Setting the first and the last layer to 8-bit
the quantized model is below!
QuantModel(
  (model): ResNet(
    (conv1): QuantModule(
      wbit=8, abit=4, disable_act_quant=False
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): ReLU(inplace=True)
    )
    (bn1): StraightThrough()
    (relu): StraightThrough()
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer2): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer3): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer4): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QuantModule(
      wbit=8, abit=4, disable_act_quant=True
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): StraightThrough()
    )
  )
)
Reconstruction for layer conv1
Start correcting 32 batches of data!
Init alpha to be FP32
Total loss:	0.052 (rec:0.039, pd:0.013, round:0.000)	b=0.00	count=500
Total loss:	0.059 (rec:0.046, pd:0.013, round:0.000)	b=0.00	count=1000
Total loss:	0.074 (rec:0.046, pd:0.028, round:0.000)	b=0.00	count=1500
Total loss:	0.062 (rec:0.049, pd:0.012, round:0.000)	b=0.00	count=2000
Total loss:	0.049 (rec:0.041, pd:0.009, round:0.000)	b=0.00	count=2500
Total loss:	0.073 (rec:0.054, pd:0.019, round:0.000)	b=0.00	count=3000
Total loss:	0.063 (rec:0.053, pd:0.010, round:0.000)	b=0.00	count=3500
Total loss:	77.617 (rec:0.047, pd:0.014, round:77.556)	b=20.00	count=4000
Total loss:	34.947 (rec:0.047, pd:0.013, round:34.887)	b=19.44	count=4500
Total loss:	32.191 (rec:0.046, pd:0.016, round:32.129)	b=18.88	count=5000
Total loss:	30.051 (rec:0.046, pd:0.015, round:29.990)	b=18.31	count=5500
Total loss:	28.293 (rec:0.044, pd:0.014, round:28.235)	b=17.75	count=6000
Total loss:	26.563 (rec:0.048, pd:0.014, round:26.501)	b=17.19	count=6500
Total loss:	25.084 (rec:0.043, pd:0.009, round:25.033)	b=16.62	count=7000
Total loss:	23.788 (rec:0.048, pd:0.011, round:23.729)	b=16.06	count=7500
Total loss:	22.317 (rec:0.049, pd:0.008, round:22.260)	b=15.50	count=8000
Total loss:	20.881 (rec:0.040, pd:0.009, round:20.832)	b=14.94	count=8500
Total loss:	19.604 (rec:0.048, pd:0.012, round:19.543)	b=14.38	count=9000
Total loss:	18.403 (rec:0.043, pd:0.011, round:18.349)	b=13.81	count=9500
Total loss:	17.163 (rec:0.041, pd:0.010, round:17.113)	b=13.25	count=10000
Total loss:	16.052 (rec:0.041, pd:0.009, round:16.001)	b=12.69	count=10500
Total loss:	14.786 (rec:0.041, pd:0.009, round:14.736)	b=12.12	count=11000
Total loss:	13.327 (rec:0.048, pd:0.012, round:13.267)	b=11.56	count=11500
Total loss:	12.178 (rec:0.044, pd:0.011, round:12.124)	b=11.00	count=12000
Total loss:	10.996 (rec:0.045, pd:0.011, round:10.940)	b=10.44	count=12500
Total loss:	9.842 (rec:0.040, pd:0.008, round:9.794)	b=9.88	count=13000
Total loss:	8.810 (rec:0.042, pd:0.012, round:8.755)	b=9.31	count=13500
Total loss:	7.436 (rec:0.044, pd:0.014, round:7.377)	b=8.75	count=14000
Total loss:	6.306 (rec:0.042, pd:0.029, round:6.235)	b=8.19	count=14500
Total loss:	5.319 (rec:0.051, pd:0.014, round:5.254)	b=7.62	count=15000
Total loss:	4.270 (rec:0.043, pd:0.012, round:4.215)	b=7.06	count=15500
Total loss:	3.118 (rec:0.045, pd:0.012, round:3.061)	b=6.50	count=16000
Total loss:	2.030 (rec:0.041, pd:0.010, round:1.979)	b=5.94	count=16500
Total loss:	0.953 (rec:0.049, pd:0.013, round:0.890)	b=5.38	count=17000
Total loss:	0.225 (rec:0.046, pd:0.011, round:0.168)	b=4.81	count=17500
Total loss:	0.125 (rec:0.050, pd:0.013, round:0.062)	b=4.25	count=18000
Total loss:	0.065 (rec:0.047, pd:0.008, round:0.010)	b=3.69	count=18500
Total loss:	0.052 (rec:0.044, pd:0.008, round:0.000)	b=3.12	count=19000
Total loss:	0.076 (rec:0.046, pd:0.029, round:0.000)	b=2.56	count=19500
Total loss:	0.070 (rec:0.054, pd:0.016, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.176 (rec:0.141, pd:0.036, round:0.000)	b=0.00	count=500
Total loss:	0.202 (rec:0.164, pd:0.038, round:0.000)	b=0.00	count=1000
Total loss:	0.213 (rec:0.185, pd:0.027, round:0.000)	b=0.00	count=1500
Total loss:	0.165 (rec:0.135, pd:0.030, round:0.000)	b=0.00	count=2000
Total loss:	0.195 (rec:0.168, pd:0.027, round:0.000)	b=0.00	count=2500
Total loss:	0.162 (rec:0.137, pd:0.025, round:0.000)	b=0.00	count=3000
Total loss:	0.167 (rec:0.143, pd:0.024, round:0.000)	b=0.00	count=3500
Total loss:	635.285 (rec:0.142, pd:0.023, round:635.120)	b=20.00	count=4000
Total loss:	345.637 (rec:0.139, pd:0.021, round:345.477)	b=19.44	count=4500
Total loss:	322.606 (rec:0.156, pd:0.031, round:322.419)	b=18.88	count=5000
Total loss:	307.269 (rec:0.148, pd:0.029, round:307.093)	b=18.31	count=5500
Total loss:	293.574 (rec:0.164, pd:0.030, round:293.379)	b=17.75	count=6000
Total loss:	281.228 (rec:0.146, pd:0.024, round:281.058)	b=17.19	count=6500
Total loss:	269.474 (rec:0.126, pd:0.017, round:269.331)	b=16.62	count=7000
Total loss:	257.495 (rec:0.133, pd:0.016, round:257.346)	b=16.06	count=7500
Total loss:	246.037 (rec:0.153, pd:0.023, round:245.862)	b=15.50	count=8000
Total loss:	234.998 (rec:0.133, pd:0.018, round:234.848)	b=14.94	count=8500
Total loss:	223.898 (rec:0.147, pd:0.022, round:223.729)	b=14.38	count=9000
Total loss:	212.623 (rec:0.144, pd:0.020, round:212.459)	b=13.81	count=9500
Total loss:	201.550 (rec:0.132, pd:0.018, round:201.399)	b=13.25	count=10000
Total loss:	190.026 (rec:0.152, pd:0.021, round:189.854)	b=12.69	count=10500
Total loss:	178.353 (rec:0.153, pd:0.021, round:178.178)	b=12.12	count=11000
Total loss:	165.842 (rec:0.139, pd:0.016, round:165.687)	b=11.56	count=11500
Total loss:	153.330 (rec:0.151, pd:0.022, round:153.156)	b=11.00	count=12000
Total loss:	140.470 (rec:0.158, pd:0.019, round:140.293)	b=10.44	count=12500
Total loss:	126.644 (rec:0.172, pd:0.021, round:126.452)	b=9.88	count=13000
Total loss:	112.779 (rec:0.160, pd:0.019, round:112.599)	b=9.31	count=13500
Total loss:	98.606 (rec:0.134, pd:0.020, round:98.452)	b=8.75	count=14000
Total loss:	84.406 (rec:0.162, pd:0.019, round:84.226)	b=8.19	count=14500
Total loss:	69.669 (rec:0.154, pd:0.020, round:69.495)	b=7.62	count=15000
Total loss:	54.189 (rec:0.181, pd:0.026, round:53.982)	b=7.06	count=15500
Total loss:	39.686 (rec:0.138, pd:0.022, round:39.527)	b=6.50	count=16000
Total loss:	25.359 (rec:0.147, pd:0.021, round:25.192)	b=5.94	count=16500
Total loss:	13.029 (rec:0.165, pd:0.028, round:12.836)	b=5.38	count=17000
Total loss:	3.513 (rec:0.141, pd:0.022, round:3.351)	b=4.81	count=17500
Total loss:	0.533 (rec:0.197, pd:0.024, round:0.313)	b=4.25	count=18000
Total loss:	0.221 (rec:0.182, pd:0.020, round:0.019)	b=3.69	count=18500
Total loss:	0.202 (rec:0.174, pd:0.029, round:0.000)	b=3.12	count=19000
Total loss:	0.157 (rec:0.134, pd:0.023, round:0.000)	b=2.56	count=19500
Total loss:	0.192 (rec:0.168, pd:0.024, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.329 (rec:0.301, pd:0.028, round:0.000)	b=0.00	count=500
Total loss:	0.351 (rec:0.324, pd:0.027, round:0.000)	b=0.00	count=1000
Total loss:	0.316 (rec:0.292, pd:0.024, round:0.000)	b=0.00	count=1500
Total loss:	0.271 (rec:0.254, pd:0.017, round:0.000)	b=0.00	count=2000
Total loss:	0.323 (rec:0.300, pd:0.023, round:0.000)	b=0.00	count=2500
Total loss:	0.278 (rec:0.261, pd:0.018, round:0.000)	b=0.00	count=3000
Total loss:	0.322 (rec:0.300, pd:0.022, round:0.000)	b=0.00	count=3500
Total loss:	653.402 (rec:0.260, pd:0.019, round:653.123)	b=20.00	count=4000
Total loss:	348.623 (rec:0.271, pd:0.017, round:348.335)	b=19.44	count=4500
Total loss:	324.739 (rec:0.287, pd:0.019, round:324.433)	b=18.88	count=5000
Total loss:	308.104 (rec:0.281, pd:0.020, round:307.803)	b=18.31	count=5500
Total loss:	294.408 (rec:0.263, pd:0.020, round:294.125)	b=17.75	count=6000
Total loss:	281.800 (rec:0.260, pd:0.014, round:281.525)	b=17.19	count=6500
Total loss:	269.677 (rec:0.266, pd:0.019, round:269.393)	b=16.62	count=7000
Total loss:	257.990 (rec:0.280, pd:0.023, round:257.687)	b=16.06	count=7500
Total loss:	246.201 (rec:0.304, pd:0.020, round:245.877)	b=15.50	count=8000
Total loss:	234.162 (rec:0.275, pd:0.019, round:233.867)	b=14.94	count=8500
Total loss:	222.226 (rec:0.395, pd:0.028, round:221.803)	b=14.38	count=9000
Total loss:	210.403 (rec:0.257, pd:0.024, round:210.122)	b=13.81	count=9500
Total loss:	198.395 (rec:0.283, pd:0.019, round:198.093)	b=13.25	count=10000
Total loss:	185.843 (rec:0.317, pd:0.019, round:185.507)	b=12.69	count=10500
Total loss:	172.755 (rec:0.265, pd:0.018, round:172.472)	b=12.12	count=11000
Total loss:	158.926 (rec:0.284, pd:0.020, round:158.622)	b=11.56	count=11500
Total loss:	145.189 (rec:0.330, pd:0.021, round:144.838)	b=11.00	count=12000
Total loss:	130.677 (rec:0.272, pd:0.016, round:130.390)	b=10.44	count=12500
Total loss:	115.810 (rec:0.268, pd:0.020, round:115.522)	b=9.88	count=13000
Total loss:	99.774 (rec:0.276, pd:0.018, round:99.481)	b=9.31	count=13500
Total loss:	84.194 (rec:0.306, pd:0.022, round:83.866)	b=8.75	count=14000
Total loss:	67.566 (rec:0.312, pd:0.019, round:67.234)	b=8.19	count=14500
Total loss:	51.523 (rec:0.297, pd:0.017, round:51.209)	b=7.62	count=15000
Total loss:	35.738 (rec:0.291, pd:0.024, round:35.422)	b=7.06	count=15500
Total loss:	21.731 (rec:0.295, pd:0.016, round:21.420)	b=6.50	count=16000
Total loss:	11.364 (rec:0.291, pd:0.016, round:11.056)	b=5.94	count=16500
Total loss:	3.528 (rec:0.284, pd:0.020, round:3.224)	b=5.38	count=17000
Total loss:	0.663 (rec:0.375, pd:0.024, round:0.264)	b=4.81	count=17500
Total loss:	0.358 (rec:0.305, pd:0.023, round:0.030)	b=4.25	count=18000
Total loss:	0.299 (rec:0.280, pd:0.016, round:0.003)	b=3.69	count=18500
Total loss:	0.310 (rec:0.290, pd:0.020, round:0.000)	b=3.12	count=19000
Total loss:	0.301 (rec:0.282, pd:0.018, round:0.000)	b=2.56	count=19500
Total loss:	0.308 (rec:0.286, pd:0.022, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.163 (rec:0.142, pd:0.021, round:0.000)	b=0.00	count=500
Total loss:	0.158 (rec:0.141, pd:0.017, round:0.000)	b=0.00	count=1000
Total loss:	0.151 (rec:0.137, pd:0.015, round:0.000)	b=0.00	count=1500
Total loss:	0.154 (rec:0.137, pd:0.017, round:0.000)	b=0.00	count=2000
Total loss:	0.161 (rec:0.144, pd:0.017, round:0.000)	b=0.00	count=2500
Total loss:	0.160 (rec:0.144, pd:0.016, round:0.000)	b=0.00	count=3000
Total loss:	0.153 (rec:0.135, pd:0.017, round:0.000)	b=0.00	count=3500
Total loss:	2105.098 (rec:0.138, pd:0.016, round:2104.944)	b=20.00	count=4000
Total loss:	1068.782 (rec:0.153, pd:0.018, round:1068.611)	b=19.44	count=4500
Total loss:	996.589 (rec:0.146, pd:0.017, round:996.426)	b=18.88	count=5000
Total loss:	944.669 (rec:0.146, pd:0.016, round:944.508)	b=18.31	count=5500
Total loss:	900.953 (rec:0.144, pd:0.014, round:900.795)	b=17.75	count=6000
Total loss:	860.258 (rec:0.144, pd:0.016, round:860.097)	b=17.19	count=6500
Total loss:	822.215 (rec:0.164, pd:0.020, round:822.031)	b=16.62	count=7000
Total loss:	786.722 (rec:0.152, pd:0.018, round:786.552)	b=16.06	count=7500
Total loss:	750.502 (rec:0.149, pd:0.016, round:750.337)	b=15.50	count=8000
Total loss:	713.387 (rec:0.157, pd:0.020, round:713.211)	b=14.94	count=8500
Total loss:	676.943 (rec:0.149, pd:0.015, round:676.780)	b=14.38	count=9000
Total loss:	638.870 (rec:0.144, pd:0.016, round:638.710)	b=13.81	count=9500
Total loss:	600.540 (rec:0.148, pd:0.016, round:600.376)	b=13.25	count=10000
Total loss:	560.968 (rec:0.160, pd:0.017, round:560.791)	b=12.69	count=10500
Total loss:	520.440 (rec:0.155, pd:0.017, round:520.268)	b=12.12	count=11000
Total loss:	478.213 (rec:0.145, pd:0.015, round:478.053)	b=11.56	count=11500
Total loss:	436.178 (rec:0.143, pd:0.014, round:436.020)	b=11.00	count=12000
Total loss:	391.827 (rec:0.149, pd:0.017, round:391.662)	b=10.44	count=12500
Total loss:	345.819 (rec:0.153, pd:0.016, round:345.650)	b=9.88	count=13000
Total loss:	298.339 (rec:0.157, pd:0.018, round:298.164)	b=9.31	count=13500
Total loss:	249.943 (rec:0.172, pd:0.019, round:249.752)	b=8.75	count=14000
Total loss:	200.530 (rec:0.158, pd:0.015, round:200.357)	b=8.19	count=14500
Total loss:	152.407 (rec:0.164, pd:0.014, round:152.229)	b=7.62	count=15000
Total loss:	106.847 (rec:0.155, pd:0.014, round:106.678)	b=7.06	count=15500
Total loss:	66.726 (rec:0.158, pd:0.016, round:66.552)	b=6.50	count=16000
Total loss:	30.656 (rec:0.158, pd:0.015, round:30.484)	b=5.94	count=16500
Total loss:	6.203 (rec:0.181, pd:0.017, round:6.004)	b=5.38	count=17000
Total loss:	0.580 (rec:0.169, pd:0.016, round:0.395)	b=4.81	count=17500
Total loss:	0.238 (rec:0.172, pd:0.013, round:0.053)	b=4.25	count=18000
Total loss:	0.222 (rec:0.175, pd:0.018, round:0.029)	b=3.69	count=18500
Total loss:	0.188 (rec:0.171, pd:0.017, round:0.000)	b=3.12	count=19000
Total loss:	0.194 (rec:0.178, pd:0.016, round:0.000)	b=2.56	count=19500
Total loss:	0.182 (rec:0.166, pd:0.016, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.275 (rec:0.259, pd:0.016, round:0.000)	b=0.00	count=500
Total loss:	0.264 (rec:0.250, pd:0.014, round:0.000)	b=0.00	count=1000
Total loss:	0.295 (rec:0.277, pd:0.019, round:0.000)	b=0.00	count=1500
Total loss:	0.262 (rec:0.246, pd:0.016, round:0.000)	b=0.00	count=2000
Total loss:	0.275 (rec:0.260, pd:0.015, round:0.000)	b=0.00	count=2500
Total loss:	0.281 (rec:0.267, pd:0.015, round:0.000)	b=0.00	count=3000
Total loss:	0.279 (rec:0.264, pd:0.015, round:0.000)	b=0.00	count=3500
Total loss:	2659.524 (rec:0.265, pd:0.015, round:2659.244)	b=20.00	count=4000
Total loss:	1316.280 (rec:0.264, pd:0.015, round:1316.001)	b=19.44	count=4500
Total loss:	1221.562 (rec:0.283, pd:0.015, round:1221.264)	b=18.88	count=5000
Total loss:	1155.345 (rec:0.268, pd:0.014, round:1155.063)	b=18.31	count=5500
Total loss:	1098.094 (rec:0.259, pd:0.013, round:1097.822)	b=17.75	count=6000
Total loss:	1046.801 (rec:0.263, pd:0.014, round:1046.523)	b=17.19	count=6500
Total loss:	996.210 (rec:0.281, pd:0.014, round:995.915)	b=16.62	count=7000
Total loss:	948.750 (rec:0.269, pd:0.015, round:948.465)	b=16.06	count=7500
Total loss:	900.627 (rec:0.266, pd:0.014, round:900.346)	b=15.50	count=8000
Total loss:	854.144 (rec:0.279, pd:0.015, round:853.850)	b=14.94	count=8500
Total loss:	807.086 (rec:0.243, pd:0.013, round:806.830)	b=14.38	count=9000
Total loss:	758.057 (rec:0.272, pd:0.013, round:757.772)	b=13.81	count=9500
Total loss:	709.726 (rec:0.288, pd:0.014, round:709.425)	b=13.25	count=10000
Total loss:	659.773 (rec:0.267, pd:0.014, round:659.492)	b=12.69	count=10500
Total loss:	608.460 (rec:0.285, pd:0.013, round:608.162)	b=12.12	count=11000
Total loss:	555.951 (rec:0.298, pd:0.013, round:555.640)	b=11.56	count=11500
Total loss:	501.759 (rec:0.263, pd:0.013, round:501.483)	b=11.00	count=12000
Total loss:	446.551 (rec:0.276, pd:0.013, round:446.262)	b=10.44	count=12500
Total loss:	389.377 (rec:0.296, pd:0.016, round:389.065)	b=9.88	count=13000
Total loss:	332.014 (rec:0.270, pd:0.014, round:331.731)	b=9.31	count=13500
Total loss:	272.893 (rec:0.278, pd:0.014, round:272.602)	b=8.75	count=14000
Total loss:	212.719 (rec:0.280, pd:0.015, round:212.424)	b=8.19	count=14500
Total loss:	153.783 (rec:0.271, pd:0.014, round:153.498)	b=7.62	count=15000
Total loss:	99.046 (rec:0.284, pd:0.015, round:98.747)	b=7.06	count=15500
Total loss:	52.358 (rec:0.282, pd:0.014, round:52.062)	b=6.50	count=16000
Total loss:	14.521 (rec:0.271, pd:0.014, round:14.236)	b=5.94	count=16500
Total loss:	1.641 (rec:0.273, pd:0.013, round:1.355)	b=5.38	count=17000
Total loss:	0.418 (rec:0.281, pd:0.014, round:0.123)	b=4.81	count=17500
Total loss:	0.315 (rec:0.261, pd:0.014, round:0.040)	b=4.25	count=18000
Total loss:	0.313 (rec:0.298, pd:0.015, round:0.000)	b=3.69	count=18500
Total loss:	0.305 (rec:0.290, pd:0.015, round:0.000)	b=3.12	count=19000
Total loss:	0.284 (rec:0.269, pd:0.015, round:0.000)	b=2.56	count=19500
Total loss:	0.302 (rec:0.287, pd:0.015, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.191 (rec:0.175, pd:0.016, round:0.000)	b=0.00	count=500
Total loss:	0.183 (rec:0.169, pd:0.014, round:0.000)	b=0.00	count=1000
Total loss:	0.186 (rec:0.172, pd:0.014, round:0.000)	b=0.00	count=1500
Total loss:	0.182 (rec:0.168, pd:0.014, round:0.000)	b=0.00	count=2000
Total loss:	0.164 (rec:0.153, pd:0.011, round:0.000)	b=0.00	count=2500
Total loss:	0.181 (rec:0.169, pd:0.012, round:0.000)	b=0.00	count=3000
Total loss:	0.174 (rec:0.162, pd:0.012, round:0.000)	b=0.00	count=3500
Total loss:	8323.822 (rec:0.167, pd:0.011, round:8323.644)	b=20.00	count=4000
Total loss:	3940.196 (rec:0.171, pd:0.011, round:3940.014)	b=19.44	count=4500
Total loss:	3655.600 (rec:0.169, pd:0.009, round:3655.422)	b=18.88	count=5000
Total loss:	3446.902 (rec:0.177, pd:0.011, round:3446.714)	b=18.31	count=5500
Total loss:	3267.399 (rec:0.175, pd:0.011, round:3267.213)	b=17.75	count=6000
Total loss:	3100.947 (rec:0.171, pd:0.011, round:3100.765)	b=17.19	count=6500
Total loss:	2943.961 (rec:0.175, pd:0.011, round:2943.775)	b=16.62	count=7000
Total loss:	2791.300 (rec:0.164, pd:0.011, round:2791.125)	b=16.06	count=7500
Total loss:	2640.497 (rec:0.169, pd:0.011, round:2640.318)	b=15.50	count=8000
Total loss:	2491.270 (rec:0.160, pd:0.011, round:2491.099)	b=14.94	count=8500
Total loss:	2342.021 (rec:0.164, pd:0.010, round:2341.847)	b=14.38	count=9000
Total loss:	2191.168 (rec:0.171, pd:0.010, round:2190.987)	b=13.81	count=9500
Total loss:	2039.575 (rec:0.172, pd:0.011, round:2039.391)	b=13.25	count=10000
Total loss:	1883.301 (rec:0.176, pd:0.011, round:1883.114)	b=12.69	count=10500
Total loss:	1725.900 (rec:0.167, pd:0.010, round:1725.722)	b=12.12	count=11000
Total loss:	1566.802 (rec:0.180, pd:0.010, round:1566.612)	b=11.56	count=11500
Total loss:	1405.088 (rec:0.171, pd:0.011, round:1404.906)	b=11.00	count=12000
Total loss:	1240.611 (rec:0.180, pd:0.011, round:1240.420)	b=10.44	count=12500
Total loss:	1073.102 (rec:0.171, pd:0.010, round:1072.921)	b=9.88	count=13000
Total loss:	902.923 (rec:0.180, pd:0.011, round:902.732)	b=9.31	count=13500
Total loss:	733.676 (rec:0.180, pd:0.010, round:733.486)	b=8.75	count=14000
Total loss:	566.331 (rec:0.195, pd:0.012, round:566.124)	b=8.19	count=14500
Total loss:	407.638 (rec:0.182, pd:0.012, round:407.444)	b=7.62	count=15000
Total loss:	262.266 (rec:0.181, pd:0.011, round:262.074)	b=7.06	count=15500
Total loss:	127.209 (rec:0.178, pd:0.010, round:127.021)	b=6.50	count=16000
Total loss:	21.101 (rec:0.182, pd:0.009, round:20.910)	b=5.94	count=16500
Total loss:	1.421 (rec:0.182, pd:0.010, round:1.228)	b=5.38	count=17000
Total loss:	0.618 (rec:0.193, pd:0.011, round:0.413)	b=4.81	count=17500
Total loss:	0.403 (rec:0.188, pd:0.011, round:0.203)	b=4.25	count=18000
Total loss:	0.266 (rec:0.194, pd:0.012, round:0.060)	b=3.69	count=18500
Total loss:	0.200 (rec:0.188, pd:0.012, round:0.000)	b=3.12	count=19000
Total loss:	0.193 (rec:0.182, pd:0.010, round:0.000)	b=2.56	count=19500
Total loss:	0.198 (rec:0.188, pd:0.010, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.275 (rec:0.260, pd:0.015, round:0.000)	b=0.00	count=500
Total loss:	0.248 (rec:0.235, pd:0.013, round:0.000)	b=0.00	count=1000
Total loss:	0.260 (rec:0.248, pd:0.012, round:0.000)	b=0.00	count=1500
Total loss:	0.258 (rec:0.248, pd:0.010, round:0.000)	b=0.00	count=2000
Total loss:	0.254 (rec:0.245, pd:0.010, round:0.000)	b=0.00	count=2500
Total loss:	0.251 (rec:0.240, pd:0.011, round:0.000)	b=0.00	count=3000
Total loss:	0.245 (rec:0.235, pd:0.009, round:0.000)	b=0.00	count=3500
Total loss:	10578.690 (rec:0.234, pd:0.009, round:10578.447)	b=20.00	count=4000
Total loss:	4888.109 (rec:0.248, pd:0.009, round:4887.853)	b=19.44	count=4500
Total loss:	4516.389 (rec:0.266, pd:0.009, round:4516.113)	b=18.88	count=5000
Total loss:	4241.198 (rec:0.233, pd:0.009, round:4240.956)	b=18.31	count=5500
Total loss:	4000.010 (rec:0.231, pd:0.009, round:3999.771)	b=17.75	count=6000
Total loss:	3777.061 (rec:0.227, pd:0.010, round:3776.824)	b=17.19	count=6500
Total loss:	3564.995 (rec:0.233, pd:0.008, round:3564.753)	b=16.62	count=7000
Total loss:	3360.019 (rec:0.241, pd:0.009, round:3359.769)	b=16.06	count=7500
Total loss:	3160.393 (rec:0.236, pd:0.008, round:3160.148)	b=15.50	count=8000
Total loss:	2961.889 (rec:0.228, pd:0.007, round:2961.653)	b=14.94	count=8500
Total loss:	2764.289 (rec:0.247, pd:0.009, round:2764.033)	b=14.38	count=9000
Total loss:	2568.624 (rec:0.236, pd:0.008, round:2568.380)	b=13.81	count=9500
Total loss:	2370.929 (rec:0.271, pd:0.008, round:2370.650)	b=13.25	count=10000
Total loss:	2175.486 (rec:0.263, pd:0.010, round:2175.214)	b=12.69	count=10500
Total loss:	1980.651 (rec:0.238, pd:0.008, round:1980.405)	b=12.12	count=11000
Total loss:	1782.661 (rec:0.244, pd:0.008, round:1782.409)	b=11.56	count=11500
Total loss:	1584.368 (rec:0.266, pd:0.010, round:1584.093)	b=11.00	count=12000
Total loss:	1387.996 (rec:0.253, pd:0.008, round:1387.735)	b=10.44	count=12500
Total loss:	1191.541 (rec:0.243, pd:0.008, round:1191.291)	b=9.88	count=13000
Total loss:	996.999 (rec:0.256, pd:0.009, round:996.734)	b=9.31	count=13500
Total loss:	802.559 (rec:0.233, pd:0.008, round:802.318)	b=8.75	count=14000
Total loss:	615.392 (rec:0.249, pd:0.008, round:615.135)	b=8.19	count=14500
Total loss:	438.788 (rec:0.249, pd:0.009, round:438.529)	b=7.62	count=15000
Total loss:	276.363 (rec:0.241, pd:0.008, round:276.115)	b=7.06	count=15500
Total loss:	134.246 (rec:0.237, pd:0.009, round:134.000)	b=6.50	count=16000
Total loss:	32.120 (rec:0.249, pd:0.009, round:31.862)	b=5.94	count=16500
Total loss:	4.884 (rec:0.255, pd:0.009, round:4.621)	b=5.38	count=17000
Total loss:	1.156 (rec:0.239, pd:0.009, round:0.908)	b=4.81	count=17500
Total loss:	0.474 (rec:0.285, pd:0.009, round:0.180)	b=4.25	count=18000
Total loss:	0.289 (rec:0.246, pd:0.010, round:0.033)	b=3.69	count=18500
Total loss:	0.258 (rec:0.250, pd:0.008, round:0.000)	b=3.12	count=19000
Total loss:	0.257 (rec:0.247, pd:0.009, round:0.000)	b=2.56	count=19500
Total loss:	0.259 (rec:0.251, pd:0.008, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.330 (rec:0.316, pd:0.014, round:0.000)	b=0.00	count=500
Total loss:	0.311 (rec:0.300, pd:0.011, round:0.000)	b=0.00	count=1000
Total loss:	0.331 (rec:0.320, pd:0.011, round:0.000)	b=0.00	count=1500
Total loss:	0.290 (rec:0.280, pd:0.010, round:0.000)	b=0.00	count=2000
Total loss:	0.276 (rec:0.267, pd:0.009, round:0.000)	b=0.00	count=2500
Total loss:	0.270 (rec:0.261, pd:0.009, round:0.000)	b=0.00	count=3000
Total loss:	0.268 (rec:0.259, pd:0.009, round:0.000)	b=0.00	count=3500
Total loss:	32350.291 (rec:0.254, pd:0.008, round:32350.029)	b=20.00	count=4000
Total loss:	14720.565 (rec:0.269, pd:0.009, round:14720.288)	b=19.44	count=4500
Total loss:	13516.015 (rec:0.260, pd:0.009, round:13515.746)	b=18.88	count=5000
Total loss:	12600.431 (rec:0.262, pd:0.008, round:12600.161)	b=18.31	count=5500
Total loss:	11774.913 (rec:0.245, pd:0.007, round:11774.660)	b=17.75	count=6000
Total loss:	11005.388 (rec:0.256, pd:0.007, round:11005.125)	b=17.19	count=6500
Total loss:	10266.197 (rec:0.242, pd:0.007, round:10265.948)	b=16.62	count=7000
Total loss:	9557.808 (rec:0.235, pd:0.008, round:9557.565)	b=16.06	count=7500
Total loss:	8868.366 (rec:0.249, pd:0.007, round:8868.110)	b=15.50	count=8000
Total loss:	8202.399 (rec:0.251, pd:0.007, round:8202.142)	b=14.94	count=8500
Total loss:	7555.786 (rec:0.249, pd:0.008, round:7555.530)	b=14.38	count=9000
Total loss:	6930.978 (rec:0.245, pd:0.008, round:6930.725)	b=13.81	count=9500
Total loss:	6316.958 (rec:0.245, pd:0.007, round:6316.706)	b=13.25	count=10000
Total loss:	5719.447 (rec:0.248, pd:0.007, round:5719.192)	b=12.69	count=10500
Total loss:	5140.816 (rec:0.256, pd:0.007, round:5140.554)	b=12.12	count=11000
Total loss:	4582.573 (rec:0.239, pd:0.007, round:4582.327)	b=11.56	count=11500
Total loss:	4034.698 (rec:0.262, pd:0.008, round:4034.428)	b=11.00	count=12000
Total loss:	3502.889 (rec:0.237, pd:0.007, round:3502.645)	b=10.44	count=12500
Total loss:	2990.838 (rec:0.253, pd:0.007, round:2990.578)	b=9.88	count=13000
Total loss:	2500.362 (rec:0.243, pd:0.007, round:2500.112)	b=9.31	count=13500
Total loss:	2026.136 (rec:0.234, pd:0.008, round:2025.894)	b=8.75	count=14000
Total loss:	1573.793 (rec:0.243, pd:0.008, round:1573.543)	b=8.19	count=14500
Total loss:	1156.496 (rec:0.244, pd:0.007, round:1156.245)	b=7.62	count=15000
Total loss:	772.826 (rec:0.253, pd:0.007, round:772.565)	b=7.06	count=15500
Total loss:	430.276 (rec:0.243, pd:0.007, round:430.026)	b=6.50	count=16000
Total loss:	159.344 (rec:0.246, pd:0.008, round:159.090)	b=5.94	count=16500
Total loss:	33.159 (rec:0.280, pd:0.008, round:32.872)	b=5.38	count=17000
Total loss:	5.204 (rec:0.262, pd:0.007, round:4.935)	b=4.81	count=17500
Total loss:	1.083 (rec:0.278, pd:0.009, round:0.797)	b=4.25	count=18000
Total loss:	0.384 (rec:0.270, pd:0.008, round:0.106)	b=3.69	count=18500
Total loss:	0.278 (rec:0.269, pd:0.009, round:0.000)	b=3.12	count=19000
Total loss:	0.267 (rec:0.259, pd:0.008, round:0.000)	b=2.56	count=19500
Total loss:	0.266 (rec:0.258, pd:0.007, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 32 batches of data!
Init alpha to be FP32
Init alpha to be FP32
Total loss:	35.493 (rec:35.477, pd:0.016, round:0.000)	b=0.00	count=500
Total loss:	30.177 (rec:30.161, pd:0.016, round:0.000)	b=0.00	count=1000
Total loss:	26.018 (rec:26.004, pd:0.014, round:0.000)	b=0.00	count=1500
Total loss:	26.025 (rec:26.012, pd:0.013, round:0.000)	b=0.00	count=2000
Total loss:	26.760 (rec:26.743, pd:0.017, round:0.000)	b=0.00	count=2500
Total loss:	25.316 (rec:25.297, pd:0.019, round:0.000)	b=0.00	count=3000
Total loss:	24.285 (rec:24.271, pd:0.014, round:0.000)	b=0.00	count=3500
Total loss:	42228.055 (rec:23.530, pd:0.018, round:42204.504)	b=20.00	count=4000
Total loss:	25857.861 (rec:23.582, pd:0.016, round:25834.264)	b=19.44	count=4500
Total loss:	24268.977 (rec:22.008, pd:0.016, round:24246.953)	b=18.88	count=5000
Total loss:	23152.857 (rec:22.636, pd:0.017, round:23130.205)	b=18.31	count=5500
Total loss:	22193.098 (rec:23.125, pd:0.018, round:22169.955)	b=17.75	count=6000
Total loss:	21307.832 (rec:22.728, pd:0.019, round:21285.086)	b=17.19	count=6500
Total loss:	20463.686 (rec:21.866, pd:0.015, round:20441.803)	b=16.62	count=7000
Total loss:	19648.174 (rec:20.032, pd:0.017, round:19628.127)	b=16.06	count=7500
Total loss:	18856.496 (rec:22.359, pd:0.019, round:18834.117)	b=15.50	count=8000
Total loss:	18068.543 (rec:21.758, pd:0.015, round:18046.770)	b=14.94	count=8500
Total loss:	17283.291 (rec:20.574, pd:0.019, round:17262.697)	b=14.38	count=9000
Total loss:	16513.410 (rec:22.268, pd:0.018, round:16491.125)	b=13.81	count=9500
Total loss:	15736.638 (rec:20.361, pd:0.018, round:15716.260)	b=13.25	count=10000
Total loss:	14950.912 (rec:20.349, pd:0.018, round:14930.545)	b=12.69	count=10500
Total loss:	14163.518 (rec:22.723, pd:0.021, round:14140.773)	b=12.12	count=11000
Total loss:	13358.124 (rec:20.819, pd:0.017, round:13337.288)	b=11.56	count=11500
Total loss:	12538.534 (rec:22.289, pd:0.017, round:12516.229)	b=11.00	count=12000
Total loss:	11699.385 (rec:21.084, pd:0.020, round:11678.281)	b=10.44	count=12500
Total loss:	10845.161 (rec:22.972, pd:0.021, round:10822.168)	b=9.88	count=13000
Total loss:	9965.806 (rec:21.377, pd:0.019, round:9944.410)	b=9.31	count=13500
Total loss:	9057.657 (rec:20.619, pd:0.015, round:9037.023)	b=8.75	count=14000
Total loss:	8128.753 (rec:20.173, pd:0.017, round:8108.563)	b=8.19	count=14500
Total loss:	7171.184 (rec:20.113, pd:0.018, round:7151.053)	b=7.62	count=15000
Total loss:	6189.375 (rec:22.064, pd:0.021, round:6167.290)	b=7.06	count=15500
Total loss:	5179.924 (rec:21.448, pd:0.015, round:5158.461)	b=6.50	count=16000
Total loss:	4143.310 (rec:21.573, pd:0.017, round:4121.720)	b=5.94	count=16500
Total loss:	3104.895 (rec:23.189, pd:0.017, round:3081.688)	b=5.38	count=17000
Total loss:	2076.117 (rec:21.508, pd:0.018, round:2054.592)	b=4.81	count=17500
Total loss:	1129.416 (rec:22.561, pd:0.022, round:1106.833)	b=4.25	count=18000
Total loss:	409.048 (rec:23.275, pd:0.018, round:385.755)	b=3.69	count=18500
Total loss:	88.341 (rec:22.056, pd:0.020, round:66.265)	b=3.12	count=19000
Total loss:	33.772 (rec:22.417, pd:0.015, round:11.341)	b=2.56	count=19500
Total loss:	26.819 (rec:22.972, pd:0.025, round:3.822)	b=2.00	count=20000
Reconstruction for layer fc
Start correcting 32 batches of data!
Init alpha to be FP32
Total loss:	21.140 (rec:21.117, pd:0.023, round:0.000)	b=0.00	count=500
Total loss:	19.611 (rec:19.588, pd:0.023, round:0.000)	b=0.00	count=1000
Total loss:	21.621 (rec:21.598, pd:0.022, round:0.000)	b=0.00	count=1500
Total loss:	17.027 (rec:17.009, pd:0.018, round:0.000)	b=0.00	count=2000
Total loss:	17.958 (rec:17.936, pd:0.021, round:0.000)	b=0.00	count=2500
Total loss:	17.720 (rec:17.704, pd:0.016, round:0.000)	b=0.00	count=3000
Total loss:	14.948 (rec:14.933, pd:0.015, round:0.000)	b=0.00	count=3500
Total loss:	4128.421 (rec:15.620, pd:0.016, round:4112.785)	b=20.00	count=4000
Total loss:	2137.748 (rec:17.166, pd:0.017, round:2120.566)	b=19.44	count=4500
Total loss:	1929.008 (rec:16.468, pd:0.019, round:1912.521)	b=18.88	count=5000
Total loss:	1763.229 (rec:14.082, pd:0.015, round:1749.132)	b=18.31	count=5500
Total loss:	1622.444 (rec:15.954, pd:0.017, round:1606.473)	b=17.75	count=6000
Total loss:	1494.064 (rec:15.952, pd:0.016, round:1478.096)	b=17.19	count=6500
Total loss:	1375.958 (rec:16.014, pd:0.017, round:1359.927)	b=16.62	count=7000
Total loss:	1268.866 (rec:18.165, pd:0.018, round:1250.684)	b=16.06	count=7500
Total loss:	1166.609 (rec:18.103, pd:0.020, round:1148.485)	b=15.50	count=8000
Total loss:	1071.821 (rec:18.566, pd:0.019, round:1053.237)	b=14.94	count=8500
Total loss:	984.383 (rec:18.891, pd:0.020, round:965.472)	b=14.38	count=9000
Total loss:	900.545 (rec:16.353, pd:0.018, round:884.174)	b=13.81	count=9500
Total loss:	822.486 (rec:17.538, pd:0.018, round:804.930)	b=13.25	count=10000
Total loss:	750.919 (rec:17.887, pd:0.018, round:733.014)	b=12.69	count=10500
Total loss:	679.700 (rec:14.762, pd:0.015, round:664.922)	b=12.12	count=11000
Total loss:	620.040 (rec:18.964, pd:0.019, round:601.058)	b=11.56	count=11500
Total loss:	557.896 (rec:18.215, pd:0.017, round:539.663)	b=11.00	count=12000
Total loss:	498.358 (rec:15.100, pd:0.016, round:483.242)	b=10.44	count=12500
Total loss:	446.653 (rec:16.529, pd:0.016, round:430.108)	b=9.88	count=13000
Total loss:	396.520 (rec:16.728, pd:0.017, round:379.775)	b=9.31	count=13500
Total loss:	347.965 (rec:16.467, pd:0.016, round:331.482)	b=8.75	count=14000
Total loss:	301.874 (rec:16.362, pd:0.015, round:285.497)	b=8.19	count=14500
Total loss:	258.208 (rec:16.062, pd:0.017, round:242.128)	b=7.62	count=15000
Total loss:	219.352 (rec:17.814, pd:0.019, round:201.518)	b=7.06	count=15500
Total loss:	178.805 (rec:17.108, pd:0.018, round:161.679)	b=6.50	count=16000
Total loss:	141.339 (rec:18.234, pd:0.018, round:123.087)	b=5.94	count=16500
Total loss:	102.686 (rec:15.478, pd:0.013, round:87.195)	b=5.38	count=17000
Total loss:	68.868 (rec:15.888, pd:0.017, round:52.963)	b=4.81	count=17500
Total loss:	43.885 (rec:18.503, pd:0.019, round:25.363)	b=4.25	count=18000
Total loss:	21.068 (rec:14.796, pd:0.016, round:6.256)	b=3.69	count=18500
Total loss:	18.964 (rec:18.576, pd:0.019, round:0.368)	b=3.12	count=19000
Total loss:	18.880 (rec:18.860, pd:0.020, round:0.000)	b=2.56	count=19500
Total loss:	17.374 (rec:17.357, pd:0.017, round:0.000)	b=2.00	count=20000
Test: [  0/782]	Time  0.590 ( 0.590)	Acc@1  87.50 ( 87.50)	Acc@5  90.62 ( 90.62)
Test: [100/782]	Time  0.274 ( 0.104)	Acc@1  82.81 ( 76.01)	Acc@5  96.88 ( 91.97)
Test: [200/782]	Time  0.262 ( 0.099)	Acc@1  78.12 ( 75.56)	Acc@5  92.19 ( 93.30)
Test: [300/782]	Time  0.354 ( 0.101)	Acc@1  76.56 ( 75.84)	Acc@5  95.31 ( 93.30)
Test: [400/782]	Time  0.155 ( 0.098)	Acc@1  67.19 ( 72.97)	Acc@5  92.19 ( 91.55)
Test: [500/782]	Time  0.265 ( 0.098)	Acc@1  68.75 ( 71.49)	Acc@5  93.75 ( 90.31)
Test: [600/782]	Time  0.242 ( 0.097)	Acc@1  76.56 ( 70.30)	Acc@5  90.62 ( 89.45)
Test: [700/782]	Time  0.135 ( 0.096)	Acc@1  67.19 ( 69.23)	Acc@5  90.62 ( 88.84)
 * Acc@1 69.154 Acc@5 88.886
Full quantization (W4A4) accuracy: 69.15399932861328
