==> Using Pytorch Dataset
Setting the first and the last layer to 8-bit
the quantized model is below!
QuantModel(
  (model): ResNet(
    (conv1): QuantModule(
      wbit=8, abit=4, disable_act_quant=False
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): ReLU(inplace=True)
    )
    (bn1): StraightThrough()
    (relu): StraightThrough()
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer2): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer3): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer4): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QuantModule(
      wbit=8, abit=4, disable_act_quant=True
      (weight_quantizer): UniformAffineQuantizer(bit=8, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): StraightThrough()
    )
  )
)
Time for init: 2.87s
Time for computing Scaler for weight: 19.53s
Reconstruction for layer conv1
Start correcting 8 batches of data!
Total loss:	19.651 (mse:4.033, mean:10.068, std:5.550)	count=499
Total loss:	23.598 (mse:4.851, mean:12.490, std:6.256)	count=499
Total loss:	22.519 (mse:4.978, mean:11.069, std:6.472)	count=499
Total loss:	20.632 (mse:4.426, mean:11.156, std:5.050)	count=499
Total loss:	23.803 (mse:5.137, mean:12.402, std:6.264)	count=499
Total loss:	20.339 (mse:4.677, mean:10.496, std:5.166)	count=499
Total loss:	21.452 (mse:4.881, mean:9.963, std:6.608)	count=499
Total loss:	22.155 (mse:4.828, mean:11.855, std:5.471)	count=499
Init alpha to be FP32
Total loss:	0.055 (rec:0.043, pd:0.012, round:0.000)	b=0.00	count=500
Total loss:	0.049 (rec:0.040, pd:0.010, round:0.000)	b=0.00	count=1000
Total loss:	0.062 (rec:0.050, pd:0.011, round:0.000)	b=0.00	count=1500
Total loss:	0.061 (rec:0.049, pd:0.012, round:0.000)	b=0.00	count=2000
Total loss:	0.049 (rec:0.040, pd:0.009, round:0.000)	b=0.00	count=2500
Total loss:	0.061 (rec:0.047, pd:0.014, round:0.000)	b=0.00	count=3000
Total loss:	0.052 (rec:0.042, pd:0.010, round:0.000)	b=0.00	count=3500
Total loss:	74.158 (rec:0.039, pd:0.010, round:74.108)	b=20.00	count=4000
Total loss:	33.562 (rec:0.042, pd:0.014, round:33.507)	b=19.44	count=4500
Total loss:	30.662 (rec:0.041, pd:0.009, round:30.611)	b=18.88	count=5000
Total loss:	28.615 (rec:0.054, pd:0.011, round:28.550)	b=18.31	count=5500
Total loss:	26.867 (rec:0.049, pd:0.011, round:26.808)	b=17.75	count=6000
Total loss:	25.156 (rec:0.041, pd:0.012, round:25.104)	b=17.19	count=6500
Total loss:	23.818 (rec:0.045, pd:0.011, round:23.762)	b=16.62	count=7000
Total loss:	22.312 (rec:0.043, pd:0.011, round:22.258)	b=16.06	count=7500
Total loss:	20.780 (rec:0.044, pd:0.009, round:20.727)	b=15.50	count=8000
Total loss:	19.347 (rec:0.036, pd:0.009, round:19.302)	b=14.94	count=8500
Total loss:	18.016 (rec:0.040, pd:0.008, round:17.967)	b=14.38	count=9000
Total loss:	16.584 (rec:0.047, pd:0.009, round:16.528)	b=13.81	count=9500
Total loss:	15.433 (rec:0.052, pd:0.010, round:15.372)	b=13.25	count=10000
Total loss:	14.224 (rec:0.048, pd:0.010, round:14.166)	b=12.69	count=10500
Total loss:	12.887 (rec:0.046, pd:0.011, round:12.831)	b=12.12	count=11000
Total loss:	11.947 (rec:0.040, pd:0.008, round:11.899)	b=11.56	count=11500
Total loss:	10.767 (rec:0.037, pd:0.015, round:10.715)	b=11.00	count=12000
Total loss:	9.697 (rec:0.050, pd:0.010, round:9.637)	b=10.44	count=12500
Total loss:	8.716 (rec:0.045, pd:0.010, round:8.660)	b=9.88	count=13000
Total loss:	7.815 (rec:0.049, pd:0.016, round:7.750)	b=9.31	count=13500
Total loss:	6.815 (rec:0.044, pd:0.012, round:6.759)	b=8.75	count=14000
Total loss:	5.641 (rec:0.037, pd:0.008, round:5.596)	b=8.19	count=14500
Total loss:	4.730 (rec:0.047, pd:0.010, round:4.673)	b=7.62	count=15000
Total loss:	3.947 (rec:0.043, pd:0.014, round:3.890)	b=7.06	count=15500
Total loss:	2.747 (rec:0.047, pd:0.013, round:2.688)	b=6.50	count=16000
Total loss:	1.740 (rec:0.045, pd:0.007, round:1.688)	b=5.94	count=16500
Total loss:	0.842 (rec:0.039, pd:0.009, round:0.794)	b=5.38	count=17000
Total loss:	0.234 (rec:0.047, pd:0.009, round:0.177)	b=4.81	count=17500
Total loss:	0.078 (rec:0.039, pd:0.009, round:0.030)	b=4.25	count=18000
Total loss:	0.064 (rec:0.043, pd:0.011, round:0.010)	b=3.69	count=18500
Total loss:	0.055 (rec:0.044, pd:0.011, round:0.000)	b=3.12	count=19000
Total loss:	0.052 (rec:0.041, pd:0.011, round:0.000)	b=2.56	count=19500
Total loss:	0.059 (rec:0.048, pd:0.011, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 8 batches of data!
Total loss:	1.289 (mse:0.183, mean:0.538, std:0.568)	count=499
Total loss:	1.726 (mse:0.232, mean:0.728, std:0.766)	count=499
Total loss:	1.659 (mse:0.220, mean:0.637, std:0.802)	count=499
Total loss:	1.376 (mse:0.187, mean:0.527, std:0.662)	count=499
Total loss:	1.917 (mse:0.264, mean:0.820, std:0.832)	count=499
Total loss:	1.400 (mse:0.182, mean:0.574, std:0.643)	count=499
Total loss:	1.823 (mse:0.265, mean:0.687, std:0.870)	count=499
Total loss:	1.573 (mse:0.202, mean:0.731, std:0.640)	count=499
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.189 (rec:0.163, pd:0.026, round:0.000)	b=0.00	count=500
Total loss:	0.168 (rec:0.147, pd:0.021, round:0.000)	b=0.00	count=1000
Total loss:	0.161 (rec:0.136, pd:0.025, round:0.000)	b=0.00	count=1500
Total loss:	0.203 (rec:0.180, pd:0.023, round:0.000)	b=0.00	count=2000
Total loss:	0.151 (rec:0.134, pd:0.017, round:0.000)	b=0.00	count=2500
Total loss:	0.162 (rec:0.140, pd:0.022, round:0.000)	b=0.00	count=3000
Total loss:	0.150 (rec:0.132, pd:0.018, round:0.000)	b=0.00	count=3500
Total loss:	628.483 (rec:0.135, pd:0.018, round:628.330)	b=20.00	count=4000
Total loss:	337.383 (rec:0.136, pd:0.015, round:337.233)	b=19.44	count=4500
Total loss:	313.430 (rec:0.136, pd:0.015, round:313.278)	b=18.88	count=5000
Total loss:	297.595 (rec:0.135, pd:0.017, round:297.442)	b=18.31	count=5500
Total loss:	283.339 (rec:0.143, pd:0.018, round:283.178)	b=17.75	count=6000
Total loss:	270.873 (rec:0.140, pd:0.016, round:270.717)	b=17.19	count=6500
Total loss:	257.910 (rec:0.143, pd:0.015, round:257.752)	b=16.62	count=7000
Total loss:	245.906 (rec:0.140, pd:0.015, round:245.751)	b=16.06	count=7500
Total loss:	235.094 (rec:0.122, pd:0.013, round:234.960)	b=15.50	count=8000
Total loss:	223.641 (rec:0.128, pd:0.015, round:223.497)	b=14.94	count=8500
Total loss:	212.125 (rec:0.122, pd:0.017, round:211.986)	b=14.38	count=9000
Total loss:	200.624 (rec:0.158, pd:0.016, round:200.450)	b=13.81	count=9500
Total loss:	188.584 (rec:0.133, pd:0.015, round:188.436)	b=13.25	count=10000
Total loss:	175.901 (rec:0.141, pd:0.018, round:175.742)	b=12.69	count=10500
Total loss:	163.284 (rec:0.145, pd:0.018, round:163.121)	b=12.12	count=11000
Total loss:	150.990 (rec:0.159, pd:0.016, round:150.815)	b=11.56	count=11500
Total loss:	138.364 (rec:0.152, pd:0.020, round:138.192)	b=11.00	count=12000
Total loss:	126.087 (rec:0.145, pd:0.015, round:125.927)	b=10.44	count=12500
Total loss:	112.412 (rec:0.140, pd:0.015, round:112.257)	b=9.88	count=13000
Total loss:	98.707 (rec:0.180, pd:0.015, round:98.512)	b=9.31	count=13500
Total loss:	84.695 (rec:0.150, pd:0.017, round:84.528)	b=8.75	count=14000
Total loss:	70.467 (rec:0.133, pd:0.013, round:70.321)	b=8.19	count=14500
Total loss:	55.398 (rec:0.171, pd:0.018, round:55.209)	b=7.62	count=15000
Total loss:	41.068 (rec:0.149, pd:0.020, round:40.898)	b=7.06	count=15500
Total loss:	27.914 (rec:0.136, pd:0.014, round:27.764)	b=6.50	count=16000
Total loss:	15.574 (rec:0.143, pd:0.013, round:15.417)	b=5.94	count=16500
Total loss:	6.435 (rec:0.150, pd:0.017, round:6.269)	b=5.38	count=17000
Total loss:	1.185 (rec:0.158, pd:0.018, round:1.008)	b=4.81	count=17500
Total loss:	0.248 (rec:0.176, pd:0.018, round:0.053)	b=4.25	count=18000
Total loss:	0.196 (rec:0.170, pd:0.016, round:0.010)	b=3.69	count=18500
Total loss:	0.207 (rec:0.191, pd:0.016, round:0.000)	b=3.12	count=19000
Total loss:	0.204 (rec:0.184, pd:0.020, round:0.000)	b=2.56	count=19500
Total loss:	0.208 (rec:0.191, pd:0.017, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 8 batches of data!
Total loss:	0.892 (mse:0.059, mean:0.475, std:0.357)	count=499
Total loss:	1.114 (mse:0.073, mean:0.571, std:0.470)	count=499
Total loss:	1.038 (mse:0.066, mean:0.484, std:0.489)	count=499
Total loss:	0.866 (mse:0.054, mean:0.437, std:0.375)	count=499
Total loss:	1.221 (mse:0.079, mean:0.620, std:0.522)	count=499
Total loss:	0.836 (mse:0.050, mean:0.435, std:0.351)	count=499
Total loss:	1.202 (mse:0.079, mean:0.520, std:0.604)	count=499
Total loss:	1.003 (mse:0.064, mean:0.571, std:0.368)	count=499
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.296 (rec:0.283, pd:0.013, round:0.000)	b=0.00	count=500
Total loss:	0.341 (rec:0.323, pd:0.018, round:0.000)	b=0.00	count=1000
Total loss:	0.289 (rec:0.276, pd:0.014, round:0.000)	b=0.00	count=1500
Total loss:	0.275 (rec:0.260, pd:0.015, round:0.000)	b=0.00	count=2000
Total loss:	0.291 (rec:0.275, pd:0.016, round:0.000)	b=0.00	count=2500
Total loss:	0.263 (rec:0.251, pd:0.012, round:0.000)	b=0.00	count=3000
Total loss:	0.269 (rec:0.255, pd:0.014, round:0.000)	b=0.00	count=3500
Total loss:	641.625 (rec:0.268, pd:0.014, round:641.343)	b=20.00	count=4000
Total loss:	335.865 (rec:0.280, pd:0.015, round:335.570)	b=19.44	count=4500
Total loss:	312.036 (rec:0.274, pd:0.014, round:311.747)	b=18.88	count=5000
Total loss:	294.824 (rec:0.280, pd:0.015, round:294.529)	b=18.31	count=5500
Total loss:	280.308 (rec:0.260, pd:0.017, round:280.031)	b=17.75	count=6000
Total loss:	267.204 (rec:0.269, pd:0.013, round:266.921)	b=17.19	count=6500
Total loss:	254.423 (rec:0.292, pd:0.014, round:254.117)	b=16.62	count=7000
Total loss:	242.039 (rec:0.249, pd:0.013, round:241.777)	b=16.06	count=7500
Total loss:	229.670 (rec:0.315, pd:0.014, round:229.341)	b=15.50	count=8000
Total loss:	217.913 (rec:0.280, pd:0.014, round:217.619)	b=14.94	count=8500
Total loss:	206.171 (rec:0.331, pd:0.015, round:205.825)	b=14.38	count=9000
Total loss:	193.494 (rec:0.282, pd:0.013, round:193.199)	b=13.81	count=9500
Total loss:	180.958 (rec:0.299, pd:0.015, round:180.644)	b=13.25	count=10000
Total loss:	168.309 (rec:0.272, pd:0.016, round:168.021)	b=12.69	count=10500
Total loss:	155.192 (rec:0.286, pd:0.015, round:154.891)	b=12.12	count=11000
Total loss:	141.238 (rec:0.296, pd:0.015, round:140.927)	b=11.56	count=11500
Total loss:	126.926 (rec:0.309, pd:0.014, round:126.603)	b=11.00	count=12000
Total loss:	112.474 (rec:0.287, pd:0.014, round:112.173)	b=10.44	count=12500
Total loss:	97.179 (rec:0.259, pd:0.012, round:96.907)	b=9.88	count=13000
Total loss:	81.539 (rec:0.286, pd:0.014, round:81.239)	b=9.31	count=13500
Total loss:	66.349 (rec:0.273, pd:0.012, round:66.064)	b=8.75	count=14000
Total loss:	50.832 (rec:0.279, pd:0.013, round:50.541)	b=8.19	count=14500
Total loss:	35.824 (rec:0.335, pd:0.011, round:35.478)	b=7.62	count=15000
Total loss:	22.571 (rec:0.279, pd:0.012, round:22.280)	b=7.06	count=15500
Total loss:	12.630 (rec:0.289, pd:0.014, round:12.327)	b=6.50	count=16000
Total loss:	4.543 (rec:0.285, pd:0.013, round:4.246)	b=5.94	count=16500
Total loss:	0.938 (rec:0.299, pd:0.017, round:0.622)	b=5.38	count=17000
Total loss:	0.421 (rec:0.325, pd:0.018, round:0.078)	b=4.81	count=17500
Total loss:	0.353 (rec:0.297, pd:0.015, round:0.040)	b=4.25	count=18000
Total loss:	0.300 (rec:0.282, pd:0.015, round:0.003)	b=3.69	count=18500
Total loss:	0.313 (rec:0.299, pd:0.014, round:0.000)	b=3.12	count=19000
Total loss:	0.303 (rec:0.292, pd:0.012, round:0.000)	b=2.56	count=19500
Total loss:	0.314 (rec:0.300, pd:0.015, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 8 batches of data!
Total loss:	1.884 (mse:0.198, mean:0.859, std:0.827)	count=499
Total loss:	2.360 (mse:0.254, mean:1.077, std:1.030)	count=499
Total loss:	2.160 (mse:0.230, mean:0.979, std:0.950)	count=499
Total loss:	1.886 (mse:0.195, mean:0.922, std:0.770)	count=499
Total loss:	2.591 (mse:0.275, mean:1.240, std:1.075)	count=499
Total loss:	2.039 (mse:0.218, mean:1.046, std:0.775)	count=499
Total loss:	2.336 (mse:0.255, mean:0.920, std:1.160)	count=499
Total loss:	2.116 (mse:0.225, mean:1.047, std:0.845)	count=499
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.150 (rec:0.137, pd:0.013, round:0.000)	b=0.00	count=500
Total loss:	0.149 (rec:0.137, pd:0.012, round:0.000)	b=0.00	count=1000
Total loss:	0.158 (rec:0.147, pd:0.011, round:0.000)	b=0.00	count=1500
Total loss:	0.160 (rec:0.148, pd:0.012, round:0.000)	b=0.00	count=2000
Total loss:	0.167 (rec:0.157, pd:0.011, round:0.000)	b=0.00	count=2500
Total loss:	0.149 (rec:0.139, pd:0.010, round:0.000)	b=0.00	count=3000
Total loss:	0.169 (rec:0.159, pd:0.010, round:0.000)	b=0.00	count=3500
Total loss:	2066.137 (rec:0.139, pd:0.008, round:2065.990)	b=20.00	count=4000
Total loss:	1028.028 (rec:0.151, pd:0.009, round:1027.868)	b=19.44	count=4500
Total loss:	949.260 (rec:0.143, pd:0.009, round:949.108)	b=18.88	count=5000
Total loss:	896.343 (rec:0.137, pd:0.008, round:896.198)	b=18.31	count=5500
Total loss:	849.018 (rec:0.138, pd:0.007, round:848.873)	b=17.75	count=6000
Total loss:	807.114 (rec:0.149, pd:0.010, round:806.955)	b=17.19	count=6500
Total loss:	767.647 (rec:0.133, pd:0.008, round:767.506)	b=16.62	count=7000
Total loss:	728.241 (rec:0.148, pd:0.008, round:728.086)	b=16.06	count=7500
Total loss:	690.120 (rec:0.134, pd:0.008, round:689.978)	b=15.50	count=8000
Total loss:	650.964 (rec:0.161, pd:0.009, round:650.794)	b=14.94	count=8500
Total loss:	611.957 (rec:0.148, pd:0.008, round:611.801)	b=14.38	count=9000
Total loss:	573.338 (rec:0.145, pd:0.009, round:573.184)	b=13.81	count=9500
Total loss:	532.594 (rec:0.145, pd:0.008, round:532.440)	b=13.25	count=10000
Total loss:	492.076 (rec:0.146, pd:0.009, round:491.921)	b=12.69	count=10500
Total loss:	450.590 (rec:0.151, pd:0.009, round:450.430)	b=12.12	count=11000
Total loss:	406.997 (rec:0.157, pd:0.008, round:406.832)	b=11.56	count=11500
Total loss:	362.029 (rec:0.157, pd:0.009, round:361.863)	b=11.00	count=12000
Total loss:	316.040 (rec:0.148, pd:0.008, round:315.884)	b=10.44	count=12500
Total loss:	268.757 (rec:0.149, pd:0.008, round:268.600)	b=9.88	count=13000
Total loss:	221.438 (rec:0.153, pd:0.008, round:221.278)	b=9.31	count=13500
Total loss:	174.417 (rec:0.164, pd:0.008, round:174.245)	b=8.75	count=14000
Total loss:	129.217 (rec:0.167, pd:0.009, round:129.042)	b=8.19	count=14500
Total loss:	88.786 (rec:0.169, pd:0.010, round:88.607)	b=7.62	count=15000
Total loss:	54.387 (rec:0.159, pd:0.008, round:54.220)	b=7.06	count=15500
Total loss:	26.560 (rec:0.172, pd:0.009, round:26.380)	b=6.50	count=16000
Total loss:	8.461 (rec:0.165, pd:0.008, round:8.288)	b=5.94	count=16500
Total loss:	0.814 (rec:0.173, pd:0.011, round:0.631)	b=5.38	count=17000
Total loss:	0.364 (rec:0.165, pd:0.009, round:0.190)	b=4.81	count=17500
Total loss:	0.235 (rec:0.166, pd:0.009, round:0.059)	b=4.25	count=18000
Total loss:	0.198 (rec:0.161, pd:0.008, round:0.029)	b=3.69	count=18500
Total loss:	0.172 (rec:0.162, pd:0.010, round:0.000)	b=3.12	count=19000
Total loss:	0.180 (rec:0.169, pd:0.011, round:0.000)	b=2.56	count=19500
Total loss:	0.178 (rec:0.167, pd:0.011, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 8 batches of data!
Total loss:	0.594 (mse:0.046, mean:0.346, std:0.203)	count=499
Total loss:	0.721 (mse:0.059, mean:0.396, std:0.265)	count=499
Total loss:	0.672 (mse:0.057, mean:0.359, std:0.256)	count=499
Total loss:	0.604 (mse:0.049, mean:0.369, std:0.185)	count=499
Total loss:	0.831 (mse:0.071, mean:0.461, std:0.299)	count=499
Total loss:	0.614 (mse:0.048, mean:0.365, std:0.200)	count=499
Total loss:	0.782 (mse:0.067, mean:0.378, std:0.337)	count=499
Total loss:	0.657 (mse:0.052, mean:0.406, std:0.199)	count=499
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.268 (rec:0.259, pd:0.009, round:0.000)	b=0.00	count=500
Total loss:	0.254 (rec:0.247, pd:0.008, round:0.000)	b=0.00	count=1000
Total loss:	0.278 (rec:0.270, pd:0.008, round:0.000)	b=0.00	count=1500
Total loss:	0.264 (rec:0.257, pd:0.008, round:0.000)	b=0.00	count=2000
Total loss:	0.251 (rec:0.243, pd:0.008, round:0.000)	b=0.00	count=2500
Total loss:	0.253 (rec:0.246, pd:0.007, round:0.000)	b=0.00	count=3000
Total loss:	0.270 (rec:0.263, pd:0.007, round:0.000)	b=0.00	count=3500
Total loss:	2581.932 (rec:0.246, pd:0.007, round:2581.678)	b=20.00	count=4000
Total loss:	1255.133 (rec:0.260, pd:0.006, round:1254.867)	b=19.44	count=4500
Total loss:	1156.399 (rec:0.269, pd:0.008, round:1156.122)	b=18.88	count=5000
Total loss:	1083.960 (rec:0.269, pd:0.007, round:1083.684)	b=18.31	count=5500
Total loss:	1023.952 (rec:0.265, pd:0.007, round:1023.680)	b=17.75	count=6000
Total loss:	968.371 (rec:0.270, pd:0.006, round:968.094)	b=17.19	count=6500
Total loss:	916.714 (rec:0.256, pd:0.007, round:916.452)	b=16.62	count=7000
Total loss:	865.827 (rec:0.255, pd:0.007, round:865.565)	b=16.06	count=7500
Total loss:	816.322 (rec:0.260, pd:0.007, round:816.055)	b=15.50	count=8000
Total loss:	768.254 (rec:0.257, pd:0.006, round:767.990)	b=14.94	count=8500
Total loss:	719.948 (rec:0.259, pd:0.007, round:719.682)	b=14.38	count=9000
Total loss:	669.206 (rec:0.276, pd:0.007, round:668.922)	b=13.81	count=9500
Total loss:	619.750 (rec:0.265, pd:0.006, round:619.479)	b=13.25	count=10000
Total loss:	570.513 (rec:0.259, pd:0.006, round:570.248)	b=12.69	count=10500
Total loss:	519.007 (rec:0.249, pd:0.006, round:518.751)	b=12.12	count=11000
Total loss:	466.111 (rec:0.256, pd:0.006, round:465.848)	b=11.56	count=11500
Total loss:	411.469 (rec:0.274, pd:0.007, round:411.188)	b=11.00	count=12000
Total loss:	356.020 (rec:0.257, pd:0.007, round:355.756)	b=10.44	count=12500
Total loss:	300.511 (rec:0.265, pd:0.007, round:300.239)	b=9.88	count=13000
Total loss:	244.258 (rec:0.274, pd:0.007, round:243.977)	b=9.31	count=13500
Total loss:	188.017 (rec:0.262, pd:0.007, round:187.747)	b=8.75	count=14000
Total loss:	136.130 (rec:0.265, pd:0.007, round:135.857)	b=8.19	count=14500
Total loss:	88.336 (rec:0.259, pd:0.006, round:88.071)	b=7.62	count=15000
Total loss:	48.150 (rec:0.253, pd:0.007, round:47.890)	b=7.06	count=15500
Total loss:	18.098 (rec:0.283, pd:0.008, round:17.807)	b=6.50	count=16000
Total loss:	2.827 (rec:0.270, pd:0.008, round:2.550)	b=5.94	count=16500
Total loss:	0.583 (rec:0.264, pd:0.006, round:0.313)	b=5.38	count=17000
Total loss:	0.337 (rec:0.273, pd:0.007, round:0.057)	b=4.81	count=17500
Total loss:	0.309 (rec:0.272, pd:0.007, round:0.030)	b=4.25	count=18000
Total loss:	0.269 (rec:0.261, pd:0.008, round:0.000)	b=3.69	count=18500
Total loss:	0.286 (rec:0.278, pd:0.007, round:0.000)	b=3.12	count=19000
Total loss:	0.277 (rec:0.270, pd:0.007, round:0.000)	b=2.56	count=19500
Total loss:	0.288 (rec:0.282, pd:0.007, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 8 batches of data!
Total loss:	2.693 (mse:0.495, mean:1.504, std:0.693)	count=499
Total loss:	3.203 (mse:0.585, mean:1.812, std:0.806)	count=499
Total loss:	3.103 (mse:0.591, mean:1.702, std:0.811)	count=499
Total loss:	2.829 (mse:0.536, mean:1.606, std:0.688)	count=499
Total loss:	3.536 (mse:0.639, mean:1.915, std:0.982)	count=499
Total loss:	2.931 (mse:0.529, mean:1.671, std:0.731)	count=499
Total loss:	3.209 (mse:0.607, mean:1.637, std:0.965)	count=499
Total loss:	3.044 (mse:0.565, mean:1.792, std:0.686)	count=499
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.159 (rec:0.151, pd:0.008, round:0.000)	b=0.00	count=500
Total loss:	0.160 (rec:0.153, pd:0.006, round:0.000)	b=0.00	count=1000
Total loss:	0.156 (rec:0.150, pd:0.006, round:0.000)	b=0.00	count=1500
Total loss:	0.156 (rec:0.150, pd:0.006, round:0.000)	b=0.00	count=2000
Total loss:	0.152 (rec:0.147, pd:0.005, round:0.000)	b=0.00	count=2500
Total loss:	0.148 (rec:0.142, pd:0.005, round:0.000)	b=0.00	count=3000
Total loss:	0.159 (rec:0.155, pd:0.005, round:0.000)	b=0.00	count=3500
Total loss:	8051.300 (rec:0.144, pd:0.004, round:8051.151)	b=20.00	count=4000
Total loss:	3746.207 (rec:0.146, pd:0.005, round:3746.057)	b=19.44	count=4500
Total loss:	3443.271 (rec:0.149, pd:0.005, round:3443.117)	b=18.88	count=5000
Total loss:	3217.388 (rec:0.147, pd:0.005, round:3217.236)	b=18.31	count=5500
Total loss:	3020.093 (rec:0.146, pd:0.005, round:3019.942)	b=17.75	count=6000
Total loss:	2837.635 (rec:0.148, pd:0.005, round:2837.482)	b=17.19	count=6500
Total loss:	2663.059 (rec:0.143, pd:0.004, round:2662.912)	b=16.62	count=7000
Total loss:	2497.248 (rec:0.145, pd:0.005, round:2497.098)	b=16.06	count=7500
Total loss:	2336.804 (rec:0.154, pd:0.004, round:2336.645)	b=15.50	count=8000
Total loss:	2177.905 (rec:0.149, pd:0.005, round:2177.751)	b=14.94	count=8500
Total loss:	2020.170 (rec:0.145, pd:0.004, round:2020.021)	b=14.38	count=9000
Total loss:	1862.985 (rec:0.154, pd:0.005, round:1862.826)	b=13.81	count=9500
Total loss:	1707.632 (rec:0.161, pd:0.005, round:1707.466)	b=13.25	count=10000
Total loss:	1551.199 (rec:0.148, pd:0.004, round:1551.047)	b=12.69	count=10500
Total loss:	1397.464 (rec:0.157, pd:0.005, round:1397.303)	b=12.12	count=11000
Total loss:	1242.592 (rec:0.153, pd:0.005, round:1242.435)	b=11.56	count=11500
Total loss:	1090.203 (rec:0.162, pd:0.004, round:1090.037)	b=11.00	count=12000
Total loss:	936.302 (rec:0.154, pd:0.004, round:936.144)	b=10.44	count=12500
Total loss:	786.102 (rec:0.153, pd:0.004, round:785.945)	b=9.88	count=13000
Total loss:	635.833 (rec:0.160, pd:0.005, round:635.668)	b=9.31	count=13500
Total loss:	491.568 (rec:0.163, pd:0.005, round:491.400)	b=8.75	count=14000
Total loss:	355.448 (rec:0.163, pd:0.005, round:355.280)	b=8.19	count=14500
Total loss:	233.655 (rec:0.154, pd:0.005, round:233.496)	b=7.62	count=15000
Total loss:	129.685 (rec:0.157, pd:0.005, round:129.523)	b=7.06	count=15500
Total loss:	44.988 (rec:0.159, pd:0.005, round:44.824)	b=6.50	count=16000
Total loss:	6.377 (rec:0.165, pd:0.005, round:6.208)	b=5.94	count=16500
Total loss:	1.275 (rec:0.167, pd:0.005, round:1.103)	b=5.38	count=17000
Total loss:	0.570 (rec:0.165, pd:0.005, round:0.400)	b=4.81	count=17500
Total loss:	0.367 (rec:0.160, pd:0.005, round:0.203)	b=4.25	count=18000
Total loss:	0.228 (rec:0.163, pd:0.005, round:0.060)	b=3.69	count=18500
Total loss:	0.162 (rec:0.157, pd:0.005, round:0.000)	b=3.12	count=19000
Total loss:	0.166 (rec:0.161, pd:0.005, round:0.000)	b=2.56	count=19500
Total loss:	0.164 (rec:0.159, pd:0.005, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 8 batches of data!
Total loss:	1.272 (mse:0.149, mean:0.798, std:0.325)	count=499
Total loss:	1.467 (mse:0.155, mean:0.958, std:0.354)	count=499
Total loss:	1.397 (mse:0.162, mean:0.863, std:0.372)	count=499
Total loss:	1.317 (mse:0.146, mean:0.864, std:0.307)	count=499
Total loss:	1.504 (mse:0.157, mean:0.889, std:0.459)	count=499
Total loss:	1.331 (mse:0.144, mean:0.828, std:0.359)	count=499
Total loss:	1.543 (mse:0.193, mean:0.936, std:0.414)	count=499
Total loss:	1.273 (mse:0.133, mean:0.844, std:0.296)	count=499
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.222 (rec:0.216, pd:0.006, round:0.000)	b=0.00	count=500
Total loss:	0.204 (rec:0.200, pd:0.005, round:0.000)	b=0.00	count=1000
Total loss:	0.209 (rec:0.205, pd:0.004, round:0.000)	b=0.00	count=1500
Total loss:	0.187 (rec:0.184, pd:0.004, round:0.000)	b=0.00	count=2000
Total loss:	0.226 (rec:0.221, pd:0.005, round:0.000)	b=0.00	count=2500
Total loss:	0.209 (rec:0.205, pd:0.004, round:0.000)	b=0.00	count=3000
Total loss:	0.206 (rec:0.202, pd:0.003, round:0.000)	b=0.00	count=3500
Total loss:	10109.671 (rec:0.205, pd:0.004, round:10109.462)	b=20.00	count=4000
Total loss:	4522.172 (rec:0.215, pd:0.004, round:4521.953)	b=19.44	count=4500
Total loss:	4130.037 (rec:0.190, pd:0.003, round:4129.843)	b=18.88	count=5000
Total loss:	3833.231 (rec:0.197, pd:0.004, round:3833.031)	b=18.31	count=5500
Total loss:	3568.141 (rec:0.193, pd:0.004, round:3567.944)	b=17.75	count=6000
Total loss:	3321.229 (rec:0.215, pd:0.004, round:3321.010)	b=17.19	count=6500
Total loss:	3088.725 (rec:0.206, pd:0.004, round:3088.516)	b=16.62	count=7000
Total loss:	2867.248 (rec:0.194, pd:0.003, round:2867.052)	b=16.06	count=7500
Total loss:	2653.789 (rec:0.180, pd:0.003, round:2653.606)	b=15.50	count=8000
Total loss:	2445.480 (rec:0.200, pd:0.003, round:2445.277)	b=14.94	count=8500
Total loss:	2246.888 (rec:0.207, pd:0.004, round:2246.677)	b=14.38	count=9000
Total loss:	2051.501 (rec:0.205, pd:0.004, round:2051.293)	b=13.81	count=9500
Total loss:	1860.153 (rec:0.196, pd:0.003, round:1859.954)	b=13.25	count=10000
Total loss:	1673.456 (rec:0.201, pd:0.003, round:1673.251)	b=12.69	count=10500
Total loss:	1490.158 (rec:0.194, pd:0.004, round:1489.961)	b=12.12	count=11000
Total loss:	1313.448 (rec:0.190, pd:0.003, round:1313.255)	b=11.56	count=11500
Total loss:	1140.458 (rec:0.209, pd:0.004, round:1140.245)	b=11.00	count=12000
Total loss:	972.077 (rec:0.208, pd:0.003, round:971.865)	b=10.44	count=12500
Total loss:	806.922 (rec:0.206, pd:0.003, round:806.713)	b=9.88	count=13000
Total loss:	648.625 (rec:0.199, pd:0.004, round:648.423)	b=9.31	count=13500
Total loss:	496.500 (rec:0.199, pd:0.003, round:496.298)	b=8.75	count=14000
Total loss:	359.095 (rec:0.200, pd:0.003, round:358.891)	b=8.19	count=14500
Total loss:	232.789 (rec:0.209, pd:0.004, round:232.576)	b=7.62	count=15000
Total loss:	125.858 (rec:0.212, pd:0.004, round:125.641)	b=7.06	count=15500
Total loss:	45.260 (rec:0.203, pd:0.004, round:45.053)	b=6.50	count=16000
Total loss:	9.482 (rec:0.195, pd:0.003, round:9.284)	b=5.94	count=16500
Total loss:	2.465 (rec:0.207, pd:0.004, round:2.255)	b=5.38	count=17000
Total loss:	0.912 (rec:0.208, pd:0.003, round:0.701)	b=4.81	count=17500
Total loss:	0.380 (rec:0.213, pd:0.004, round:0.163)	b=4.25	count=18000
Total loss:	0.242 (rec:0.205, pd:0.004, round:0.033)	b=3.69	count=18500
Total loss:	0.205 (rec:0.202, pd:0.003, round:0.000)	b=3.12	count=19000
Total loss:	0.208 (rec:0.204, pd:0.004, round:0.000)	b=2.56	count=19500
Total loss:	0.210 (rec:0.206, pd:0.004, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Start correcting 8 batches of data!
Total loss:	3.894 (mse:0.728, mean:2.203, std:0.963)	count=499
Total loss:	4.502 (mse:0.847, mean:2.663, std:0.992)	count=499
Total loss:	4.690 (mse:0.963, mean:2.693, std:1.034)	count=499
Total loss:	4.136 (mse:0.751, mean:2.451, std:0.935)	count=499
Total loss:	4.564 (mse:0.866, mean:2.538, std:1.160)	count=499
Total loss:	4.194 (mse:0.779, mean:2.432, std:0.983)	count=499
Total loss:	4.332 (mse:0.890, mean:2.406, std:1.035)	count=499
Total loss:	4.097 (mse:0.726, mean:2.405, std:0.966)	count=499
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.243 (rec:0.237, pd:0.007, round:0.000)	b=0.00	count=500
Total loss:	0.208 (rec:0.202, pd:0.006, round:0.000)	b=0.00	count=1000
Total loss:	0.185 (rec:0.180, pd:0.005, round:0.000)	b=0.00	count=1500
Total loss:	0.188 (rec:0.183, pd:0.005, round:0.000)	b=0.00	count=2000
Total loss:	0.169 (rec:0.165, pd:0.004, round:0.000)	b=0.00	count=2500
Total loss:	0.174 (rec:0.170, pd:0.004, round:0.000)	b=0.00	count=3000
Total loss:	0.160 (rec:0.156, pd:0.004, round:0.000)	b=0.00	count=3500
Total loss:	31510.930 (rec:0.143, pd:0.004, round:31510.783)	b=20.00	count=4000
Total loss:	13771.670 (rec:0.150, pd:0.004, round:13771.517)	b=19.44	count=4500
Total loss:	12561.126 (rec:0.175, pd:0.004, round:12560.947)	b=18.88	count=5000
Total loss:	11635.441 (rec:0.152, pd:0.004, round:11635.285)	b=18.31	count=5500
Total loss:	10813.637 (rec:0.152, pd:0.004, round:10813.480)	b=17.75	count=6000
Total loss:	10041.338 (rec:0.135, pd:0.003, round:10041.199)	b=17.19	count=6500
Total loss:	9309.442 (rec:0.138, pd:0.003, round:9309.301)	b=16.62	count=7000
Total loss:	8611.466 (rec:0.158, pd:0.003, round:8611.305)	b=16.06	count=7500
Total loss:	7937.460 (rec:0.153, pd:0.004, round:7937.304)	b=15.50	count=8000
Total loss:	7285.232 (rec:0.152, pd:0.004, round:7285.076)	b=14.94	count=8500
Total loss:	6653.137 (rec:0.158, pd:0.004, round:6652.975)	b=14.38	count=9000
Total loss:	6033.598 (rec:0.143, pd:0.003, round:6033.452)	b=13.81	count=9500
Total loss:	5434.300 (rec:0.138, pd:0.003, round:5434.159)	b=13.25	count=10000
Total loss:	4858.001 (rec:0.136, pd:0.003, round:4857.862)	b=12.69	count=10500
Total loss:	4300.505 (rec:0.146, pd:0.003, round:4300.356)	b=12.12	count=11000
Total loss:	3759.700 (rec:0.163, pd:0.004, round:3759.532)	b=11.56	count=11500
Total loss:	3239.982 (rec:0.156, pd:0.004, round:3239.823)	b=11.00	count=12000
Total loss:	2741.776 (rec:0.161, pd:0.004, round:2741.610)	b=10.44	count=12500
Total loss:	2270.741 (rec:0.145, pd:0.003, round:2270.593)	b=9.88	count=13000
Total loss:	1821.594 (rec:0.150, pd:0.003, round:1821.440)	b=9.31	count=13500
Total loss:	1404.904 (rec:0.147, pd:0.003, round:1404.754)	b=8.75	count=14000
Total loss:	1024.978 (rec:0.153, pd:0.003, round:1024.822)	b=8.19	count=14500
Total loss:	694.660 (rec:0.155, pd:0.003, round:694.502)	b=7.62	count=15000
Total loss:	411.376 (rec:0.159, pd:0.004, round:411.213)	b=7.06	count=15500
Total loss:	185.095 (rec:0.160, pd:0.003, round:184.932)	b=6.50	count=16000
Total loss:	49.804 (rec:0.150, pd:0.003, round:49.651)	b=5.94	count=16500
Total loss:	10.869 (rec:0.155, pd:0.004, round:10.711)	b=5.38	count=17000
Total loss:	2.337 (rec:0.145, pd:0.003, round:2.189)	b=4.81	count=17500
Total loss:	0.798 (rec:0.193, pd:0.004, round:0.600)	b=4.25	count=18000
Total loss:	0.250 (rec:0.140, pd:0.003, round:0.106)	b=3.69	count=18500
Total loss:	0.154 (rec:0.151, pd:0.003, round:0.000)	b=3.12	count=19000
Total loss:	0.146 (rec:0.143, pd:0.004, round:0.000)	b=2.56	count=19500
Total loss:	0.164 (rec:0.160, pd:0.004, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Start correcting 8 batches of data!
Total loss:	3.677 (mse:0.628, mean:2.292, std:0.757)	count=499
Total loss:	3.870 (mse:0.602, mean:2.512, std:0.756)	count=499
Total loss:	4.098 (mse:0.776, mean:2.552, std:0.770)	count=499
Total loss:	3.791 (mse:0.578, mean:2.486, std:0.727)	count=499
Total loss:	4.094 (mse:0.721, mean:2.494, std:0.880)	count=499
Total loss:	3.639 (mse:0.583, mean:2.320, std:0.736)	count=499
Total loss:	3.778 (mse:0.687, mean:2.343, std:0.748)	count=499
Total loss:	3.990 (mse:0.629, mean:2.596, std:0.765)	count=499
Init alpha to be FP32
Init alpha to be FP32
Total loss:	18.830 (rec:18.818, pd:0.012, round:0.000)	b=0.00	count=500
Total loss:	15.210 (rec:15.198, pd:0.012, round:0.000)	b=0.00	count=1000
Total loss:	15.293 (rec:15.276, pd:0.017, round:0.000)	b=0.00	count=1500
Total loss:	13.796 (rec:13.779, pd:0.017, round:0.000)	b=0.00	count=2000
Total loss:	13.039 (rec:13.020, pd:0.019, round:0.000)	b=0.00	count=2500
Total loss:	11.877 (rec:11.862, pd:0.015, round:0.000)	b=0.00	count=3000
Total loss:	10.835 (rec:10.822, pd:0.013, round:0.000)	b=0.00	count=3500
Total loss:	41683.543 (rec:10.771, pd:0.014, round:41672.758)	b=20.00	count=4000
Total loss:	24934.414 (rec:11.144, pd:0.015, round:24923.256)	b=19.44	count=4500
Total loss:	23387.609 (rec:10.969, pd:0.015, round:23376.625)	b=18.88	count=5000
Total loss:	22296.955 (rec:11.210, pd:0.020, round:22285.727)	b=18.31	count=5500
Total loss:	21355.896 (rec:9.637, pd:0.017, round:21346.242)	b=17.75	count=6000
Total loss:	20476.379 (rec:9.765, pd:0.015, round:20466.602)	b=17.19	count=6500
Total loss:	19647.328 (rec:9.929, pd:0.018, round:19637.381)	b=16.62	count=7000
Total loss:	18839.785 (rec:10.417, pd:0.018, round:18829.352)	b=16.06	count=7500
Total loss:	18042.158 (rec:10.669, pd:0.018, round:18031.471)	b=15.50	count=8000
Total loss:	17254.188 (rec:10.584, pd:0.015, round:17243.590)	b=14.94	count=8500
Total loss:	16471.723 (rec:10.623, pd:0.022, round:16461.078)	b=14.38	count=9000
Total loss:	15692.443 (rec:9.342, pd:0.017, round:15683.084)	b=13.81	count=9500
Total loss:	14906.643 (rec:9.576, pd:0.015, round:14897.052)	b=13.25	count=10000
Total loss:	14114.487 (rec:9.564, pd:0.015, round:14104.908)	b=12.69	count=10500
Total loss:	13310.139 (rec:9.553, pd:0.022, round:13300.564)	b=12.12	count=11000
Total loss:	12491.091 (rec:9.662, pd:0.014, round:12481.414)	b=11.56	count=11500
Total loss:	11658.229 (rec:9.458, pd:0.018, round:11648.752)	b=11.00	count=12000
Total loss:	10810.620 (rec:9.601, pd:0.019, round:10801.000)	b=10.44	count=12500
Total loss:	9943.652 (rec:10.185, pd:0.015, round:9933.452)	b=9.88	count=13000
Total loss:	9052.209 (rec:8.562, pd:0.017, round:9043.631)	b=9.31	count=13500
Total loss:	8140.984 (rec:8.935, pd:0.014, round:8132.035)	b=8.75	count=14000
Total loss:	7213.327 (rec:10.176, pd:0.014, round:7203.137)	b=8.19	count=14500
Total loss:	6260.995 (rec:9.275, pd:0.021, round:6251.699)	b=7.62	count=15000
Total loss:	5287.011 (rec:9.394, pd:0.018, round:5277.600)	b=7.06	count=15500
Total loss:	4301.384 (rec:9.841, pd:0.017, round:4291.526)	b=6.50	count=16000
Total loss:	3302.749 (rec:10.115, pd:0.011, round:3292.623)	b=5.94	count=16500
Total loss:	2333.431 (rec:10.048, pd:0.014, round:2323.369)	b=5.38	count=17000
Total loss:	1434.799 (rec:10.098, pd:0.017, round:1424.684)	b=4.81	count=17500
Total loss:	688.395 (rec:10.757, pd:0.012, round:677.626)	b=4.25	count=18000
Total loss:	201.991 (rec:10.717, pd:0.021, round:191.252)	b=3.69	count=18500
Total loss:	37.378 (rec:10.477, pd:0.019, round:26.882)	b=3.12	count=19000
Total loss:	16.794 (rec:10.732, pd:0.014, round:6.049)	b=2.56	count=19500
Total loss:	12.491 (rec:10.757, pd:0.018, round:1.715)	b=2.00	count=20000
Reconstruction for layer fc
Start correcting 8 batches of data!
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Init alpha to be FP32
Total loss:	15.590 (rec:15.574, pd:0.016, round:0.000)	b=0.00	count=500
Total loss:	19.271 (rec:19.248, pd:0.023, round:0.000)	b=0.00	count=1000
Total loss:	15.248 (rec:15.231, pd:0.016, round:0.000)	b=0.00	count=1500
Total loss:	11.571 (rec:11.560, pd:0.011, round:0.000)	b=0.00	count=2000
Total loss:	15.058 (rec:15.038, pd:0.020, round:0.000)	b=0.00	count=2500
Total loss:	14.279 (rec:14.265, pd:0.015, round:0.000)	b=0.00	count=3000
Total loss:	11.875 (rec:11.862, pd:0.013, round:0.000)	b=0.00	count=3500
Total loss:	3611.748 (rec:12.431, pd:0.012, round:3599.305)	b=20.00	count=4000
Total loss:	1656.018 (rec:10.788, pd:0.011, round:1645.219)	b=19.44	count=4500
Total loss:	1466.558 (rec:12.580, pd:0.014, round:1453.964)	b=18.88	count=5000
Total loss:	1317.300 (rec:12.878, pd:0.013, round:1304.408)	b=18.31	count=5500
Total loss:	1190.808 (rec:14.086, pd:0.016, round:1176.706)	b=17.75	count=6000
Total loss:	1076.863 (rec:13.233, pd:0.013, round:1063.618)	b=17.19	count=6500
Total loss:	977.815 (rec:12.876, pd:0.014, round:964.926)	b=16.62	count=7000
Total loss:	890.000 (rec:15.674, pd:0.017, round:874.309)	b=16.06	count=7500
Total loss:	808.423 (rec:14.724, pd:0.016, round:793.683)	b=15.50	count=8000
Total loss:	731.390 (rec:12.359, pd:0.011, round:719.019)	b=14.94	count=8500
Total loss:	664.940 (rec:11.721, pd:0.012, round:653.207)	b=14.38	count=9000
Total loss:	603.229 (rec:12.144, pd:0.014, round:591.070)	b=13.81	count=9500
Total loss:	549.718 (rec:14.483, pd:0.019, round:535.216)	b=13.25	count=10000
Total loss:	494.921 (rec:12.025, pd:0.012, round:482.884)	b=12.69	count=10500
Total loss:	446.091 (rec:11.225, pd:0.011, round:434.855)	b=12.12	count=11000
Total loss:	405.607 (rec:15.138, pd:0.017, round:390.452)	b=11.56	count=11500
Total loss:	365.045 (rec:15.046, pd:0.017, round:349.983)	b=11.00	count=12000
Total loss:	323.000 (rec:11.340, pd:0.012, round:311.648)	b=10.44	count=12500
Total loss:	287.198 (rec:11.511, pd:0.012, round:275.674)	b=9.88	count=13000
Total loss:	252.956 (rec:11.181, pd:0.011, round:241.764)	b=9.31	count=13500
Total loss:	220.908 (rec:12.101, pd:0.012, round:208.795)	b=8.75	count=14000
Total loss:	190.925 (rec:12.234, pd:0.013, round:178.678)	b=8.19	count=14500
Total loss:	165.763 (rec:14.460, pd:0.019, round:151.284)	b=7.62	count=15000
Total loss:	138.380 (rec:14.040, pd:0.018, round:124.323)	b=7.06	count=15500
Total loss:	111.541 (rec:12.552, pd:0.013, round:98.976)	b=6.50	count=16000
Total loss:	87.161 (rec:13.406, pd:0.015, round:73.740)	b=5.94	count=16500
Total loss:	62.443 (rec:11.283, pd:0.013, round:51.147)	b=5.38	count=17000
Total loss:	42.870 (rec:12.036, pd:0.011, round:30.822)	b=4.81	count=17500
Total loss:	28.519 (rec:14.149, pd:0.015, round:14.355)	b=4.25	count=18000
Total loss:	16.598 (rec:12.962, pd:0.013, round:3.623)	b=3.69	count=18500
Total loss:	13.189 (rec:13.023, pd:0.014, round:0.152)	b=3.12	count=19000
Total loss:	13.518 (rec:13.503, pd:0.015, round:0.000)	b=2.56	count=19500
Total loss:	13.680 (rec:13.665, pd:0.016, round:0.000)	b=2.00	count=20000
Test: [   0/3125]	Time  0.385 ( 0.385)	Acc@1  81.25 ( 81.25)	Acc@5  93.75 ( 93.75)
Test: [ 100/3125]	Time  0.023 ( 0.019)	Acc@1  50.00 ( 83.11)	Acc@5  87.50 ( 94.74)
Test: [ 200/3125]	Time  0.011 ( 0.017)	Acc@1  50.00 ( 75.19)	Acc@5  87.50 ( 91.95)
Test: [ 300/3125]	Time  0.026 ( 0.016)	Acc@1  93.75 ( 76.76)	Acc@5 100.00 ( 92.50)
Test: [ 400/3125]	Time  0.014 ( 0.016)	Acc@1  93.75 ( 76.45)	Acc@5 100.00 ( 92.10)
Test: [ 500/3125]	Time  0.031 ( 0.016)	Acc@1  93.75 ( 77.61)	Acc@5 100.00 ( 92.78)
Test: [ 600/3125]	Time  0.010 ( 0.015)	Acc@1  62.50 ( 76.25)	Acc@5  93.75 ( 92.71)
Test: [ 700/3125]	Time  0.009 ( 0.015)	Acc@1  75.00 ( 76.14)	Acc@5  87.50 ( 92.97)
Test: [ 800/3125]	Time  0.020 ( 0.015)	Acc@1  87.50 ( 75.62)	Acc@5 100.00 ( 93.12)
Test: [ 900/3125]	Time  0.006 ( 0.015)	Acc@1  93.75 ( 75.31)	Acc@5 100.00 ( 93.08)
Test: [1000/3125]	Time  0.019 ( 0.015)	Acc@1  75.00 ( 75.36)	Acc@5 100.00 ( 93.11)
Test: [1100/3125]	Time  0.013 ( 0.015)	Acc@1  93.75 ( 75.85)	Acc@5 100.00 ( 93.30)
Test: [1200/3125]	Time  0.008 ( 0.015)	Acc@1  87.50 ( 75.71)	Acc@5  93.75 ( 93.19)
Test: [1300/3125]	Time  0.006 ( 0.015)	Acc@1  81.25 ( 75.31)	Acc@5 100.00 ( 92.94)
Test: [1400/3125]	Time  0.022 ( 0.015)	Acc@1  81.25 ( 74.40)	Acc@5 100.00 ( 92.55)
Test: [1500/3125]	Time  0.006 ( 0.015)	Acc@1  68.75 ( 73.64)	Acc@5  87.50 ( 91.93)
Test: [1600/3125]	Time  0.010 ( 0.015)	Acc@1  75.00 ( 72.65)	Acc@5  93.75 ( 91.47)
Test: [1700/3125]	Time  0.012 ( 0.015)	Acc@1  81.25 ( 71.98)	Acc@5  87.50 ( 91.07)
Test: [1800/3125]	Time  0.009 ( 0.015)	Acc@1  87.50 ( 71.98)	Acc@5 100.00 ( 90.97)
Test: [1900/3125]	Time  0.009 ( 0.015)	Acc@1  81.25 ( 71.62)	Acc@5  87.50 ( 90.64)
Test: [2000/3125]	Time  0.034 ( 0.015)	Acc@1  68.75 ( 71.10)	Acc@5  93.75 ( 90.21)
Test: [2100/3125]	Time  0.010 ( 0.015)	Acc@1  81.25 ( 70.80)	Acc@5  93.75 ( 90.02)
Test: [2200/3125]	Time  0.006 ( 0.015)	Acc@1  75.00 ( 70.47)	Acc@5  87.50 ( 89.78)
Test: [2300/3125]	Time  0.016 ( 0.015)	Acc@1  68.75 ( 70.21)	Acc@5  75.00 ( 89.58)
Test: [2400/3125]	Time  0.013 ( 0.015)	Acc@1  75.00 ( 69.92)	Acc@5 100.00 ( 89.39)
Test: [2500/3125]	Time  0.009 ( 0.015)	Acc@1  87.50 ( 69.55)	Acc@5  93.75 ( 89.18)
Test: [2600/3125]	Time  0.011 ( 0.015)	Acc@1  93.75 ( 69.43)	Acc@5 100.00 ( 89.06)
Test: [2700/3125]	Time  0.007 ( 0.015)	Acc@1  75.00 ( 69.04)	Acc@5 100.00 ( 88.83)
Test: [2800/3125]	Time  0.009 ( 0.015)	Acc@1  87.50 ( 68.84)	Acc@5  93.75 ( 88.73)
Test: [2900/3125]	Time  0.017 ( 0.015)	Acc@1  50.00 ( 68.59)	Acc@5 100.00 ( 88.61)
Test: [3000/3125]	Time  0.017 ( 0.015)	Acc@1  62.50 ( 68.75)	Acc@5  81.25 ( 88.70)
Test: [3100/3125]	Time  0.018 ( 0.015)	Acc@1 100.00 ( 68.74)	Acc@5 100.00 ( 88.74)
 * Acc@1 68.766 Acc@5 88.748
Full quantization (W4A4) accuracy: 68.76599884033203
