START : 2024-06-05 23:35:26

General parameters for data and model
- seed = 1005 (default = 1005)
- arch = resnet18
- batch_size = 16 (default = 64)
- workers = 8 (default = 4)
- data_path = data/ImageNet

Quantization parameters
- n_bits_w = 4 (default = 4)
- channel_wise = True (default = True)
- n_bits_a = 4 (default = 4)
- disable_8bit_head_stem = not use (action = 'store_true')

Weight calibration parameters
- num_samples = 64 (default = 1024)
- iters_w = 100 (default = 20000)
- weight = 0.01 (default = 0.01)
- keep_cpu = not use (action = 'store_true')
- b_start = 20 (default = 20)
- b_end = 2 (default = 2)
- warmup = 0.2 (default = 0.2)

Activation calibration parameters
- lr = 4e-5 (default = 4e-5)
- init_wmode = mse (default = 'mse', choices = ['minmax', 'mse', 'minmax_scale'])
- init_amode = mse (default = 'mse', choices = ['minmax', 'mse', 'minmax_scale'])
- prob = 0.5 (default = 0.5)
- input_prob = 0.5 (default = 0.5)
- lamb_r = 0.1 (default = 0.1)
- T = 4.0 (default = 4.0)
- bn_lr = 1e-3 (default = 1e-3)
- lamb_c = 0.02 (default = 0.02)

-------------------------------------------------------------------------------------------

==> Using Pytorch Dataset
the quantized model is below!
QuantModel(
  (model): ResNet(
    (conv1): QuantModule(
      wbit=4, abit=4, disable_act_quant=False
      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): ReLU(inplace=True)
    )
    (bn1): StraightThrough()
    (relu): StraightThrough()
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer2): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer3): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (layer4): Sequential(
      (0): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (downsample): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
      (1): QuantBasicBlock(
        (conv1): QuantModule(
          wbit=4, abit=4, disable_act_quant=False
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): ReLU(inplace=True)
        )
        (conv2): QuantModule(
          wbit=4, abit=4, disable_act_quant=True
          (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
          (norm_function): StraightThrough()
          (activation_function): StraightThrough()
        )
        (activation_function): ReLU(inplace=True)
        (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): QuantModule(
      wbit=4, abit=4, disable_act_quant=True
      (weight_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (act_quantizer): UniformAffineQuantizer(bit=4, is_training=False, inited=True)
      (norm_function): StraightThrough()
      (activation_function): StraightThrough()
    )
  )
)
2D 0
2D 1
2D 1
2D 2
2D 2
2D 3
2D 3
2D 3
2D 4
2D 4
2D 5
2D 5
2D 5
2D 6
2D 6
2D 7
2D 7
2D 7
2D 8
Using Min/Max quantization
2D 8
Using Min/Max quantization
2D 8
Using Min/Max quantization
Reconstruction for layer conv1
Start correcting 2 batches of data!
Total loss:	19.651 (mse:4.033, mean:10.068, std:5.550)	count=499
Total loss:	23.598 (mse:4.851, mean:12.490, std:6.256)	count=499
1D
Init alpha to be FP32
Reconstruction for block 0
Start correcting 2 batches of data!
Total loss:	1.289 (mse:0.183, mean:0.538, std:0.568)	count=499
Total loss:	1.726 (mse:0.232, mean:0.728, std:0.766)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 1
Start correcting 2 batches of data!
Total loss:	0.892 (mse:0.059, mean:0.475, std:0.357)	count=499
Total loss:	1.114 (mse:0.073, mean:0.571, std:0.470)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 0
Start correcting 2 batches of data!
Total loss:	1.884 (mse:0.198, mean:0.859, std:0.827)	count=499
Total loss:	2.360 (mse:0.254, mean:1.077, std:1.030)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 1
Start correcting 2 batches of data!
Total loss:	0.594 (mse:0.046, mean:0.346, std:0.203)	count=499
Total loss:	0.721 (mse:0.059, mean:0.396, std:0.265)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 0
Start correcting 2 batches of data!
Total loss:	2.693 (mse:0.495, mean:1.504, std:0.693)	count=499
Total loss:	3.203 (mse:0.585, mean:1.812, std:0.806)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 1
Start correcting 2 batches of data!
Total loss:	1.272 (mse:0.149, mean:0.798, std:0.325)	count=499
Total loss:	1.467 (mse:0.155, mean:0.958, std:0.354)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 0
Start correcting 2 batches of data!
Total loss:	3.894 (mse:0.728, mean:2.203, std:0.963)	count=499
Total loss:	4.502 (mse:0.847, mean:2.663, std:0.992)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for block 1
Start correcting 2 batches of data!
Total loss:	3.677 (mse:0.628, mean:2.292, std:0.757)	count=499
Total loss:	3.870 (mse:0.602, mean:2.512, std:0.756)	count=499
1D
1D
Init alpha to be FP32
Init alpha to be FP32
Reconstruction for layer fc
Start correcting 2 batches of data!
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Total loss:	0.000 (mse:0.000, mean:0.000, std:0.000)	count=499
Init alpha to be FP32
Test: [   0/3125]	Time  0.362 ( 0.362)	Acc@1  81.25 ( 81.25)	Acc@5  93.75 ( 93.75)
Test: [ 100/3125]	Time  0.016 ( 0.019)	Acc@1  37.50 ( 79.89)	Acc@5  81.25 ( 93.38)
Test: [ 200/3125]	Time  0.019 ( 0.017)	Acc@1  56.25 ( 71.49)	Acc@5  87.50 ( 90.39)
Test: [ 300/3125]	Time  0.009 ( 0.016)	Acc@1  93.75 ( 73.05)	Acc@5  93.75 ( 90.95)
Test: [ 400/3125]	Time  0.016 ( 0.016)	Acc@1  75.00 ( 73.05)	Acc@5 100.00 ( 90.74)
Test: [ 500/3125]	Time  0.013 ( 0.016)	Acc@1  87.50 ( 74.56)	Acc@5 100.00 ( 91.52)
Test: [ 600/3125]	Time  0.008 ( 0.015)	Acc@1  56.25 ( 73.34)	Acc@5  93.75 ( 91.42)
Test: [ 700/3125]	Time  0.025 ( 0.015)	Acc@1  56.25 ( 72.86)	Acc@5  87.50 ( 91.66)
Test: [ 800/3125]	Time  0.019 ( 0.015)	Acc@1  68.75 ( 72.44)	Acc@5 100.00 ( 91.80)
Test: [ 900/3125]	Time  0.007 ( 0.015)	Acc@1  81.25 ( 72.34)	Acc@5 100.00 ( 91.82)
Test: [1000/3125]	Time  0.017 ( 0.015)	Acc@1  75.00 ( 72.40)	Acc@5 100.00 ( 91.86)
Test: [1100/3125]	Time  0.007 ( 0.015)	Acc@1  87.50 ( 73.01)	Acc@5 100.00 ( 92.08)
Test: [1200/3125]	Time  0.007 ( 0.015)	Acc@1  87.50 ( 72.97)	Acc@5  93.75 ( 92.00)
Test: [1300/3125]	Time  0.013 ( 0.015)	Acc@1  81.25 ( 72.49)	Acc@5 100.00 ( 91.68)
Test: [1400/3125]	Time  0.009 ( 0.015)	Acc@1  56.25 ( 71.63)	Acc@5  93.75 ( 91.19)
Test: [1500/3125]	Time  0.011 ( 0.015)	Acc@1  68.75 ( 70.94)	Acc@5  87.50 ( 90.58)
Test: [1600/3125]	Time  0.031 ( 0.015)	Acc@1  75.00 ( 70.02)	Acc@5  87.50 ( 90.02)
Test: [1700/3125]	Time  0.014 ( 0.015)	Acc@1  62.50 ( 69.39)	Acc@5  87.50 ( 89.54)
Test: [1800/3125]	Time  0.009 ( 0.015)	Acc@1  87.50 ( 69.41)	Acc@5 100.00 ( 89.40)
Test: [1900/3125]	Time  0.019 ( 0.015)	Acc@1  68.75 ( 69.02)	Acc@5  81.25 ( 89.00)
Test: [2000/3125]	Time  0.014 ( 0.015)	Acc@1  68.75 ( 68.53)	Acc@5  87.50 ( 88.58)
Test: [2100/3125]	Time  0.010 ( 0.015)	Acc@1  81.25 ( 68.09)	Acc@5  93.75 ( 88.34)
Test: [2200/3125]	Time  0.008 ( 0.015)	Acc@1  75.00 ( 67.75)	Acc@5  87.50 ( 88.10)
Test: [2300/3125]	Time  0.006 ( 0.015)	Acc@1  62.50 ( 67.48)	Acc@5  81.25 ( 87.91)
Test: [2400/3125]	Time  0.014 ( 0.015)	Acc@1  68.75 ( 67.16)	Acc@5  87.50 ( 87.63)
Test: [2500/3125]	Time  0.006 ( 0.015)	Acc@1  87.50 ( 66.87)	Acc@5  93.75 ( 87.42)
Test: [2600/3125]	Time  0.006 ( 0.015)	Acc@1  93.75 ( 66.73)	Acc@5  93.75 ( 87.33)
Test: [2700/3125]	Time  0.012 ( 0.015)	Acc@1  75.00 ( 66.38)	Acc@5 100.00 ( 87.06)
Test: [2800/3125]	Time  0.009 ( 0.015)	Acc@1  81.25 ( 66.17)	Acc@5 100.00 ( 86.94)
Test: [2900/3125]	Time  0.009 ( 0.015)	Acc@1  43.75 ( 65.88)	Acc@5  87.50 ( 86.83)
Test: [3000/3125]	Time  0.006 ( 0.015)	Acc@1  56.25 ( 66.04)	Acc@5  87.50 ( 86.95)
Test: [3100/3125]	Time  0.016 ( 0.015)	Acc@1 100.00 ( 66.04)	Acc@5 100.00 ( 86.97)
 * Acc@1 66.078 Acc@5 86.996
Full quantization (W4A4) accuracy: 66.07799530029297
END : 2024-06-05 23:37:30
